{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Urban morphometrics\n",
    "\n",
    "Morpohometric assessment measure wide range of characters of urban form to derive a complex description of built-up patterns composed of enclosed tessellation, buildings and street network.\n",
    "\n",
    "All algorithms used within this notebook are part of `momepy` Python toolkit and can be used from there. We have extracted them from `momepy`, adapted for `dask` and `pygeos` and used in raw form tailored directly to our use case. The algorithms which were enhanced are pushed back to momepy and will be part of `momepy` 0.4.0.\n",
    "\n",
    "All steps within this notebook are parallelised using `dask`. The first part, which measures aspects of individual elements (does not require to know the context) uses pre-release of `dask-geopandas`. The rest uses `dask` to manage parallel iteration over geo-chunks with single-core algorithms. \n",
    "\n",
    "Some functions are imported from a `momepy_utils.py` file stored wihtin this directory. Those are either helper functions taken directly from momepy or their enhanced versions, all which will be included in the next release of momepy:\n",
    "\n",
    "- `get_edge_ratios` is implemented in momepy 0.4.0 as `get_network_ratio`\n",
    "- `get_nodes` is included in `get_node_id`\n",
    "- remaining functions have been used to refactor existing momepy classes.\n",
    "\n",
    "\n",
    "## Individual elements\n",
    "\n",
    "Note: Requires dask-geopandas and current master of geopandas to support dask version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+git://github.com/jsignell/dask-geopandas.git\n",
    "# !pip install git+git://github.com/geopandas/geopandas.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "from time import time\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import dask_geopandas as dask_geopandas\n",
    "import geopandas\n",
    "import libpysal\n",
    "import momepy\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pygeos\n",
    "import scipy\n",
    "from tqdm.notebook import tqdm\n",
    "from dask.distributed import Client, LocalCluster, as_completed\n",
    "from libpysal.weights import Queen\n",
    "from momepy_utils import (\n",
    "    _circle_radius,\n",
    "    centroid_corner,\n",
    "    elongation,\n",
    "    get_corners,\n",
    "    get_edge_ratios,\n",
    "    get_nodes,\n",
    "    solar_orientation_poly,\n",
    "    squareness,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using a single machine wihtin this notebook with 14 cores, so we start local dask cluster with 14 workers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:36097</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>14</li>\n",
       "  <li><b>Cores: </b>28</li>\n",
       "  <li><b>Memory: </b>134.91 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:36097' processes=14 threads=28, memory=134.91 GB>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Client(LocalCluster(n_workers=14))\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dask-geopandas` is still under development and raises few warnigns at the moment, all which can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore', message='.*initial implementation of Parquet.*')\n",
    "warnings.filterwarnings('ignore', message='.*Assigning CRS to a GeoDataFrame without a geometry*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring buildings and enclosed cells\n",
    "\n",
    "In the first step, we iterate over geo-chunks, merge enclosed tessellation and buildings to a single `geopandas.GeoDataFrame` and convert it to `dask.GeoDataFrame`. The rest of the code is mostly an extraction from momepy source code adapted for dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk_id in tqdm(range(103), total=103):\n",
    "    \n",
    "    # Load data and merge them together\n",
    "    blg = geopandas.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/buildings/blg_{chunk_id}.pq\")\n",
    "    tess = geopandas.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/tessellation/tess_{chunk_id}.pq\")\n",
    "    \n",
    "    blg = blg.rename_geometry('buildings')\n",
    "    tess = tess.rename_geometry('tessellation')\n",
    "\n",
    "    df = tess.merge(blg, on='uID', how='left')\n",
    "    \n",
    "    # Convert to dask.GeoDataFrame\n",
    "    ddf = dask_geopandas.from_geopandas(df, npartitions=14)\n",
    "    \n",
    "    ## Measure morphometric characters\n",
    "    # Building area\n",
    "    ddf['sdbAre'] = ddf.buildings.area\n",
    "    \n",
    "    # Building perimeter\n",
    "    ddf['sdbPer'] = ddf.buildings.length\n",
    "    \n",
    "    # Courtyard area\n",
    "    exterior_area = ddf.buildings.map_partitions(lambda series: pygeos.area(pygeos.polygons(series.exterior.values.data)), meta='float')\n",
    "    ddf['sdbCoA'] = exterior_area - ddf['sdbAre']\n",
    "\n",
    "    # Circular compactness\n",
    "    hull = ddf.buildings.convex_hull.exterior\n",
    "\n",
    "    radius = hull.apply(lambda g: _circle_radius(list(g.coords)) if g is not None else None, meta='float')\n",
    "    ddf['ssbCCo'] = ddf['sdbAre'] / (np.pi * radius ** 2)\n",
    "\n",
    "    # Corners\n",
    "    ddf['ssbCor'] = ddf.buildings.apply(lambda g: get_corners(g), meta='float')\n",
    "\n",
    "    # Squareness\n",
    "    ddf['ssbSqu'] = ddf.buildings.apply(lambda g: squareness(g), meta='float')\n",
    "    \n",
    "    # Equivalent rectangular index\n",
    "    bbox = ddf.buildings.apply(lambda g: g.minimum_rotated_rectangle if g is not None else None, meta=geopandas.GeoSeries())\n",
    "    ddf['ssbERI'] = (ddf['sdbAre'] / bbox.area).pow(1./2) * (bbox.length / ddf['sdbPer'])\n",
    "\n",
    "    # Elongation\n",
    "    ddf['ssbElo'] = bbox.map_partitions(lambda s: elongation(s), meta='float')\n",
    "    \n",
    "    # Centroid corner mean distance and deviation\n",
    "    def _centroid_corner(series):\n",
    "        ccd = series.apply(lambda g: centroid_corner(g))\n",
    "        return pd.DataFrame(ccd.to_list(), index=series.index)\n",
    "\n",
    "    \n",
    "    ddf[['ssbCCM', 'ssbCCD']] = ddf.buildings.map_partitions(_centroid_corner, meta=pd.DataFrame({0: [0.1], 1: [1.1]}))\n",
    "    \n",
    "    # Solar orientation\n",
    "    ddf['stbOri'] = bbox.apply(lambda g: solar_orientation_poly(g), meta='float')\n",
    "    \n",
    "    # Tessellation longest axis length\n",
    "    hull = ddf.tessellation.convex_hull.exterior\n",
    "\n",
    "    ddf['sdcLAL'] = hull.apply(lambda g: _circle_radius(list(g.coords)), meta='float') * 2\n",
    "    \n",
    "    # Tessellation area\n",
    "    ddf['sdcAre'] = ddf.tessellation.area\n",
    "    \n",
    "    # Circular compactness\n",
    "    radius = hull.apply(lambda g: _circle_radius(list(g.coords)), meta='float')\n",
    "    ddf['sscCCo'] = ddf['sdcAre'] / (np.pi * radius ** 2)\n",
    "    \n",
    "    # Equivalent rectangular index\n",
    "    bbox = ddf.tessellation.apply(lambda g: g.minimum_rotated_rectangle, meta=geopandas.GeoSeries())\n",
    "    ddf['sscERI'] = (ddf['sdcAre'] / bbox.area).pow(1./2) * (bbox.length / ddf.tessellation.length)\n",
    "    \n",
    "    # Solar orientation\n",
    "    ddf['stcOri'] = bbox.apply(lambda g: solar_orientation_poly(g), meta='float')\n",
    "    \n",
    "    # Covered area ratio\n",
    "    ddf['sicCAR'] = ddf['sdbAre'] / ddf['sdcAre']\n",
    "    \n",
    "    # Building-cell alignment\n",
    "    ddf['stbCeA'] = (ddf['stbOri'] - ddf['stcOri']).abs()\n",
    "    \n",
    "    # Compute all characters using dask\n",
    "    df = ddf.compute()\n",
    "    \n",
    "    # Save to parquet file\n",
    "    df.to_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/cells/cells_{chunk_id}.pq\")\n",
    "    client.restart()\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring enclosures\n",
    "\n",
    "All enclosures are loaded as a single dask.GeoDataFrame and measured at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Load data\n",
    "encl = dask_geopandas.read_parquet(\"../../urbangrammar_samba/spatial_signatures/enclosures/encl_*.pq\")\n",
    "\n",
    "# Area\n",
    "encl['ldeAre'] = encl.geometry.area\n",
    "\n",
    "# Perimeter\n",
    "encl['ldePer'] = encl.geometry.length\n",
    "\n",
    "# Circular compacntess\n",
    "hull = encl.geometry.convex_hull.exterior\n",
    "\n",
    "radius = hull.apply(lambda g: _circle_radius(list(g.coords)) if g is not None else None, meta='float')\n",
    "encl['lseCCo'] = encl['ldeAre'] / (np.pi * radius ** 2)\n",
    "\n",
    "# Equivalent rectangular index\n",
    "bbox = encl.geometry.apply(lambda g: g.minimum_rotated_rectangle if g is not None else None, meta=geopandas.GeoSeries())\n",
    "encl['lseERI'] = (encl['ldeAre'] / bbox.area).pow(1./2) * (bbox.length / encl['ldePer'])\n",
    "\n",
    "# Compactness-weighted axis\n",
    "longest_axis = hull.apply(lambda g: _circle_radius(list(g.coords)), meta='float') * 2\n",
    "encl['lseCWA'] = longest_axis * ((4 / np.pi) - (16 * encl['ldeAre']) / ((encl['ldePer']) ** 2))\n",
    "\n",
    "# Solar orientation\n",
    "encl['lteOri'] = bbox.apply(lambda g: solar_orientation_poly(g), meta='float')\n",
    "\n",
    "# Compute data and return geopandas.GeoDataFrame\n",
    "encl_df = encl.compute()\n",
    "\n",
    "# Weighted number of neighbors\n",
    "inp, res = encl_df.sindex.query_bulk(encl_df.geometry, predicate='intersects')\n",
    "indices, counts = np.unique(inp, return_counts=True)\n",
    "encl_df['neighbors'] = counts - 1\n",
    "encl_df['lteWNB'] = encl_df['neighbors'] / encl_df['ldePer']\n",
    "\n",
    "# Load complete enclosed tessellation as a dask.GeoDataFrame\n",
    "tess = dd.read_parquet(\"../../urbangrammar_samba/spatial_signatures/tessellation/tess_*.pq\")\n",
    "\n",
    "# Measure weighted cells within enclosure\n",
    "encl_counts = tess.groupby('enclosureID').count().compute()\n",
    "merged = encl_df[['enclosureID', 'ldeAre']].merge(encl_counts[['geometry']], how='left', on='enclosureID')\n",
    "encl_df['lieWCe'] = merged['geometry'] / merged['ldeAre']\n",
    "\n",
    "# Save data to parquet\n",
    "encl_df.drop(columns='geometry').to_parquet(\"../../urbangrammar_samba/spatial_signatures/morphometrics/enclosures.pq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now close dask client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate spatial weights (W)\n",
    "\n",
    "Subsequent steps will require understanding of the context of each tessellation cell in a form of spatial weights matrices (Queen contiguity and Queen contiguty of inclusive 3rd order). We generate them beforehand and store as `npz` files representing sparse matrix.\n",
    "\n",
    "Each geo-chunk is loaded together with relevant cross-chunk tessellation cells (to avoid edge effect). We use dask to parallelise the iteration. Number of workers is smaller now to ensure enough memory for each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = 8\n",
    "client = Client(LocalCluster(n_workers=workers, threads_per_worker=1))\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to specify a function doing the processing itself, where the only attribure is the `chunk_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_w(chunk_id):\n",
    "    # load cells of a chunk\n",
    "    cells = geopandas.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/cells/cells_{chunk_id}.pq\")\n",
    "    \n",
    "    # add neighbouring cells from other chunks\n",
    "    cross_chunk_cells = []\n",
    "    \n",
    "    for chunk, inds in cross_chunk.loc[chunk_id].indices.iteritems():\n",
    "        add_cells = geopandas.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/cells/cells_{chunk}.pq\").iloc[inds]\n",
    "        cross_chunk_cells.append(add_cells)\n",
    "    \n",
    "    df = cells.append(pd.concat(cross_chunk_cells, ignore_index=True), ignore_index=True)\n",
    "\n",
    "    w = libpysal.weights.Queen.from_dataframe(df, geom_col='tessellation')\n",
    "    w3 = momepy.sw_high(k=3, weights=w)\n",
    "    \n",
    "    scipy.sparse.save_npz(f\"../../urbangrammar_samba/spatial_signatures/weights/w_{chunk_id}.npz\", w.sparse)\n",
    "    scipy.sparse.save_npz(f\"../../urbangrammar_samba/spatial_signatures/weights/w3_{chunk_id}.npz\", w3.sparse)\n",
    "    \n",
    "    return f\"Chunk {chunk_id} processed sucessfully.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use dask to iterate over all 103 chunks. The following script sends first 8 chunks to dask together and then submits a new chunk as soon as any of previous finishes (courtesy of Matthew Rocklin). That way we process only 8 chunks at once ensuring that we the cluster will not run out of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "inputs = iter(range(103))\n",
    "futures = [client.submit(generate_w, next(inputs)) for i in range(workers)]\n",
    "ac = as_completed(futures)\n",
    "for finished_future in ac:\n",
    "    # submit new future \n",
    "    try:\n",
    "        new_future = client.submit(generate_w, next(inputs))\n",
    "        ac.add(new_future)\n",
    "    except StopIteration:\n",
    "        pass\n",
    "    print(finished_future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial distribution and network analysis\n",
    "\n",
    "To measure spatial distribution of we use single-core algorithm and parallelise iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = 8\n",
    "client = Client(LocalCluster(n_workers=workers, threads_per_worker=1))\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to load street network data from PostGIS datatabase, so we establish a connection which will be used within the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_chunk = pd.read_parquet('../../urbangrammar_samba/spatial_signatures/cross-chunk_indices.pq')\n",
    "chunks = geopandas.read_parquet('../../urbangrammar_samba/spatial_signatures/local_auth_chunks.pq')\n",
    "\n",
    "user = os.environ.get('DB_USER')\n",
    "pwd = os.environ.get('DB_PWD')\n",
    "host = os.environ.get('DB_HOST')\n",
    "port = os.environ.get('DB_PORT')\n",
    "\n",
    "db_connection_url = f\"postgres+psycopg2://{user}:{pwd}@{host}:{port}/built_env\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the same function below we measure spatial distribution of elements and network-based characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(chunk_id):\n",
    "    # load cells of a chunk\n",
    "    cells = geopandas.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/cells/cells_{chunk_id}.pq\")\n",
    "    cells['keep'] = True\n",
    "    \n",
    "    # add neighbouring cells from other chunks\n",
    "    cross_chunk_cells = []\n",
    "    \n",
    "    for chunk, inds in cross_chunk.loc[chunk_id].indices.iteritems():\n",
    "        add_cells = geopandas.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/cells/cells_{chunk}.pq\").iloc[inds]\n",
    "        add_cells['keep'] = False\n",
    "        cross_chunk_cells.append(add_cells)\n",
    "    \n",
    "    df = cells.append(pd.concat(cross_chunk_cells, ignore_index=True), ignore_index=True)\n",
    "\n",
    "    # read W\n",
    "    w = libpysal.weights.WSP(scipy.sparse.load_npz(f\"../../urbangrammar_samba/spatial_signatures/weights/w_{chunk_id}.npz\")).to_W()\n",
    "    \n",
    "    # alignment\n",
    "    def alignment(x, orientation='stbOri'):\n",
    "        orientations = df[orientation].iloc[w.neighbors[x]]\n",
    "        return abs(orientations - df[orientation].iloc[x]).mean()\n",
    "    \n",
    "    df['mtbAli'] = [alignment(x) for x in range(len(df))]\n",
    "\n",
    "    # mean neighbour distance\n",
    "    def neighbor_distance(x):\n",
    "        geom = df.buildings.iloc[x]\n",
    "        if geom is None:\n",
    "            return np.nan\n",
    "        return df.buildings.iloc[w.neighbors[x]].distance(df.buildings.iloc[x]).mean()\n",
    "\n",
    "    df['mtbNDi'] = [neighbor_distance(x) for x in range(len(df))]\n",
    "    \n",
    "    # weighted neighbours\n",
    "    df['mtcWNe'] = pd.Series([w.cardinalities[x] for x in range(len(df))], index=df.index) / df.tessellation.length\n",
    "    \n",
    "    # area covered by neighbours\n",
    "    def area_covered(x, area='sdcAre'):\n",
    "        neighbours = [x]\n",
    "        neighbours += w.neighbors[x]\n",
    "\n",
    "        return df[area].iloc[neighbours].sum()\n",
    "\n",
    "    df['mdcAre'] = [area_covered(x) for x in range(len(df))]\n",
    "    \n",
    "    # read W3\n",
    "    w3 = libpysal.weights.WSP(scipy.sparse.load_npz(f\"../../urbangrammar_samba/spatial_signatures/weights/w3_{chunk_id}.npz\")).to_W()\n",
    "      \n",
    "    # weighted reached enclosures\n",
    "    def weighted_reached_enclosures(x, area='sdcAre', enclosure_id='enclosureID'):\n",
    "        neighbours = [x]\n",
    "        neighbours += w3.neighbors[x]\n",
    "\n",
    "        vicinity = df[[area, enclosure_id]].iloc[neighbours]\n",
    "\n",
    "        return vicinity[enclosure_id].unique().shape[0] / vicinity[area].sum()\n",
    "    \n",
    "    df['ltcWRE'] = [weighted_reached_enclosures(x) for x in range(len(df))]\n",
    "    \n",
    "    # mean interbuilding distance\n",
    "    # define adjacency list from lipysal\n",
    "    adj_list = w.to_adjlist(remove_symmetric=False)\n",
    "    adj_list[\"weight\"] = (\n",
    "        df.buildings.iloc[adj_list.focal]\n",
    "        .reset_index(drop=True)\n",
    "        .distance(df.buildings.iloc[adj_list.neighbor].reset_index(drop=True)).values\n",
    "    )\n",
    "\n",
    "    G = nx.from_pandas_edgelist(\n",
    "            adj_list, source=\"focal\", target=\"neighbor\", edge_attr=\"weight\"\n",
    "        )\n",
    "    ibd = []\n",
    "    for i in range(len(df)):\n",
    "        try:\n",
    "            sub = nx.ego_graph(G, i, radius=3)\n",
    "            ibd.append(np.nanmean([x[-1] for x in list(sub.edges.data('weight'))]))\n",
    "        except:\n",
    "            ibd.append(np.nan)\n",
    "\n",
    "    df['ltbIBD'] = ibd\n",
    "    \n",
    "    # Reached neighbors and area on 3 topological steps on tessellation\n",
    "    df['ltcRea'] = [w3.cardinalities[i] for i in range(len(df))]\n",
    "    df['ltcAre'] = [df.sdcAre.iloc[w3.neighbors[i]].sum() for i in range(len(df))]\n",
    "\n",
    "    # Save cells to parquet keeping only within-chunk data not the additional neighboring\n",
    "    df[df['keep']].drop(columns=['keep']).to_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/cells/cells_{chunk_id}.pq\")\n",
    "\n",
    "    # Load street network for an extended chunk area\n",
    "    chunk_area = chunks.geometry.iloc[chunk_id].buffer(5000)  # we extend the area by 5km to minimise edge effect\n",
    "    engine = create_engine(db_connection_url)\n",
    "    sql = f\"SELECT * FROM openroads_200803_topological WHERE ST_Intersects(geometry, ST_GeomFromText('{chunk_area.wkt}',27700))\"\n",
    "    streets = geopandas.read_postgis(sql, engine, geom_col='geometry')\n",
    "    \n",
    "    # Street profile (measures width, width deviation and openness)\n",
    "    sp = street_profile(streets, blg)\n",
    "    streets['sdsSPW'] = sp[0]\n",
    "    streets['sdsSWD'] = sp[1]\n",
    "    streets['sdsSPO'] = sp[2]\n",
    "    \n",
    "    # Street segment length\n",
    "    streets['sdsLen'] = streets.length\n",
    "    \n",
    "    # Street segment linearity\n",
    "    streets['sssLin'] = momepy.Linearity(streets).series\n",
    "    \n",
    "    # Convert geopadnas.GeoDataFrame to networkx.Graph for network analysis\n",
    "    G = momepy.gdf_to_nx(streets)\n",
    "    \n",
    "    # Node degree\n",
    "    G = momepy.node_degree(G)\n",
    "    \n",
    "    # Subgraph analysis (meshedness, proportion of 0, 3 and 4 way intersections, local closeness)\n",
    "    G = momepy.subgraph(\n",
    "        G,\n",
    "        radius=5,\n",
    "        meshedness=True,\n",
    "        cds_length=False,\n",
    "        mode=\"sum\",\n",
    "        degree=\"degree\",\n",
    "        length=\"mm_len\",\n",
    "        mean_node_degree=False,\n",
    "        proportion={0: True, 3: True, 4: True},\n",
    "        cyclomatic=False,\n",
    "        edge_node_ratio=False,\n",
    "        gamma=False,\n",
    "        local_closeness=True,\n",
    "        closeness_weight=\"mm_len\",\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Cul-de-sac length\n",
    "    G = momepy.cds_length(G, radius=3, name=\"ldsCDL\", verbose=False)\n",
    "    \n",
    "    # Square clustering\n",
    "    G = momepy.clustering(G, name=\"xcnSCl\")\n",
    "    \n",
    "    # Mean node distance\n",
    "    G = momepy.mean_node_dist(G, name=\"mtdMDi\", verbose=False)\n",
    "    \n",
    "    # Convert networkx.Graph back to GeoDataFrames and W (denoting relationships between nodes)\n",
    "    nodes, edges, sw = momepy.nx_to_gdf(G, spatial_weights=True)\n",
    "    \n",
    "    # Generate inclusive higher order weights\n",
    "    edges_w3 = momepy.sw_high(k=3, gdf=edges)\n",
    "    \n",
    "    # Mean segment length\n",
    "    edges[\"ldsMSL\"] = momepy.SegmentsLength(edges, spatial_weights=edges_w3, mean=True, verbose=False).series\n",
    "    \n",
    "    # Generate inclusive higher order weights\n",
    "    nodes_w5 = momepy.sw_high(k=5, weights=sw)\n",
    "    \n",
    "    # Node density\n",
    "    nodes[\"lddNDe\"] = momepy.NodeDensity(nodes, edges, nodes_w5, verbose=False).series\n",
    "    \n",
    "    # Weighter node density\n",
    "    nodes[\"linWID\"] = momepy.NodeDensity(nodes, edges, nodes_w5, weighted=True, node_degree=\"degree\", verbose=False).series\n",
    "    \n",
    "    # Save to parquets\n",
    "    edges.to_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/edges/edges_{chunk_id}.pq\")\n",
    "    nodes.to_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/nodes/nodes_{chunk_id}.pq\")\n",
    "\n",
    "\n",
    "    return f\"Chunk {chunk_id} processed sucessfully.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we use dask to iterate over all 103 chunks. The following script sends first 8 chunks to dask together and then submits a new chunk as soon as any of previous finishes. That way we process only 8 chunks at once ensuring that we the cluster will not run out of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = iter(range(103))\n",
    "futures = [client.submit(measure, next(inputs)) for i in range(workers)]\n",
    "ac = as_completed(futures)\n",
    "for finished_future in ac:\n",
    "    # submit new future \n",
    "    try:\n",
    "        new_future = client.submit(measure, next(inputs))\n",
    "        ac.add(new_future)\n",
    "    except StopIteration:\n",
    "        pass\n",
    "    print(finished_future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link elements together\n",
    "\n",
    "For the further analysis, we need to link data measured on individual elements together. We link cells to edges based on the proportion of overlap (if a cell intersects more than one edge) and nodes based on proximity (with a restriction - node has to be on linked edge). Enclosures are linked based on enclosure ID.\n",
    "\n",
    "As above, we define a single-core function and use dask to manage parallel iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link(chunk_id):\n",
    "    s = time()\n",
    "    cells = geopandas.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/cells/cells_{chunk_id}.pq\")\n",
    "    edges = geopandas.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/edges/edges_{chunk_id}.pq\")\n",
    "    nodes = geopandas.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/nodes/nodes_{chunk_id}.pq\")\n",
    "    \n",
    "    cells['edgeID'] = get_edge_ratios(cells, edges)\n",
    "    cells['nodeID'] = get_nodes(cells, nodes, edges, 'nodeID', 'edgeID', 'node_start', 'node_end')\n",
    "    \n",
    "    characters = ['sdsSPW', 'sdsSWD', 'sdsSPO', 'sdsLen', 'sssLin', 'ldsMSL']\n",
    "    l = []\n",
    "    for d in cells.edgeID:\n",
    "        l.append((edges.iloc[list(d.keys())][characters].multiply(list(d.values()), axis='rows')).sum(axis=0))\n",
    "    cells[characters] = pd.DataFrame(l, index=cells.index)\n",
    "    \n",
    "    cells = cells.merge(nodes.drop(columns=['geometry']), on='nodeID', how='left')\n",
    "    cells = cells.rename({'degree': 'mtdDeg', 'meshedness': 'lcdMes', 'proportion_3': 'linP3W', 'proportion_4': 'linP4W',\n",
    "                     'proportion_0': 'linPDE', 'local_closeness': 'lcnClo'}, axis='columns')\n",
    "    \n",
    "    cells['edgeID_keys'] = cells.edgeID.apply(lambda d: list(d.keys()))\n",
    "    cells['edgeID_values'] = cells.edgeID.apply(lambda d: list(d.values()))\n",
    "    \n",
    "    cells.drop(columns='edgeID').to_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/cells/cells_{chunk_id}.pq\")\n",
    "    \n",
    "    return f\"Chunk {chunk_id} processed sucessfully in {time() - s} seconds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = 14\n",
    "client = Client(LocalCluster(n_workers=workers, threads_per_worker=1))\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "inputs = iter(range(103))\n",
    "futures = [client.submit(link, next(inputs)) for i in range(workers)]\n",
    "ac = as_completed(futures)\n",
    "for finished_future in ac:\n",
    "    # submit new future \n",
    "    try:\n",
    "        new_future = client.submit(link, next(inputs))\n",
    "        ac.add(new_future)\n",
    "    except StopIteration:\n",
    "        pass\n",
    "    print(finished_future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enclosures are linked via simple attribute join and since the operation is does not require any computation, it is done as a simple loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enclosures = pd.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/enclosures.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk_id in range(103):\n",
    "    s = time()\n",
    "    cells = geopandas.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/cells/cells_{chunk_id}.pq\")\n",
    "    \n",
    "    cells = cells.merge(enclosures.drop(columns=['neighbors']), on='enclosureID', how='left')\n",
    "    \n",
    "    cells.to_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/cells/cells_{chunk_id}.pq\")\n",
    "    \n",
    "    print(f\"Chunk {chunk_id} processed sucessfully in {time() - s} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inter-element characters\n",
    "\n",
    "The remaining morphometric characters are based on a relations between multiple elements. The implementation mirrors the approach above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = 8\n",
    "client = Client(LocalCluster(n_workers=workers, threads_per_worker=1))\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(chunk_id):\n",
    "    s = time()\n",
    "    # Load data\n",
    "    cells = geopandas.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/cells/cells_{chunk_id}.pq\")\n",
    "    edges = geopandas.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/edges/edges_{chunk_id}.pq\")\n",
    "    nodes = geopandas.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/nodes/nodes_{chunk_id}.pq\")\n",
    "    \n",
    "    # Street Alignment\n",
    "    edges['orient'] = momepy.Orientation(edges, verbose=False).series\n",
    "    edges['edgeID'] = range(len(edges))\n",
    "    keys = cells.edgeID_values.apply(lambda a: np.argmax(a))\n",
    "    cells['edgeID_primary'] = [inds[i] for inds, i in zip(cells.edgeID_keys, keys)]\n",
    "    cells['stbSAl'] = momepy.StreetAlignment(cells, \n",
    "                                             edges, \n",
    "                                             'stbOri', \n",
    "                                             left_network_id='edgeID_primary', \n",
    "                                             right_network_id='edgeID').series\n",
    "   \n",
    "    # Area Covered by each edge\n",
    "    vals = {x:[] for x in range(len(edges))}\n",
    "    for i, keys in enumerate(cells.edgeID_keys):\n",
    "        for k in keys:\n",
    "            vals[k].append(i)\n",
    "    area_sums = []\n",
    "    for inds in vals.values():\n",
    "        area_sums.append(cells.sdcAre.iloc[inds].sum())\n",
    "    edges['sdsAre'] = area_sums\n",
    "    \n",
    "    # Building per meter\n",
    "    bpm = []\n",
    "    for inds, l in zip(vals.values(), edges.sdsLen):\n",
    "        bpm.append(cells.buildings.iloc[inds].notna().sum() / l if len(inds) > 0 else 0)\n",
    "    edges['sisBpM'] = bpm\n",
    "    \n",
    "    # Cell area\n",
    "    nodes['sddAre'] = nodes.nodeID.apply(lambda nid: cells[cells.nodeID == nid].sdcAre.sum())\n",
    "    \n",
    "    # Area covered by neighboring edges + count of reached cells\n",
    "    edges_W = Queen.from_dataframe(edges)\n",
    "    \n",
    "    areas = []\n",
    "    reached_cells = []\n",
    "    for i in range(len(edges)):\n",
    "        neighbors = [i] + edges_W.neighbors[i]\n",
    "    #     areas\n",
    "        areas.append(edges.sdsAre.iloc[neighbors].sum())\n",
    "    #     reached cells\n",
    "        ids = []\n",
    "        for n in neighbors:\n",
    "             ids += vals[n]\n",
    "        reached_cells.append(len(set(ids)))\n",
    "\n",
    "    edges['misCel'] = reached_cells\n",
    "    edges['mdsAre'] = areas\n",
    "    \n",
    "    # Area covered by neighboring (3 steps) edges + count of reached cells\n",
    "    edges_W3 = momepy.sw_high(k=3, weights=edges_W)\n",
    "    \n",
    "    areas = []\n",
    "    reached_cells = []\n",
    "    for i in range(len(edges)):\n",
    "        neighbors = [i] + edges_W3.neighbors[i]\n",
    "    #     areas\n",
    "        areas.append(edges.sdsAre.iloc[neighbors].sum())\n",
    "    #     reached cells\n",
    "        ids = []\n",
    "        for n in neighbors:\n",
    "             ids += vals[n]\n",
    "        reached_cells.append(len(set(ids)))\n",
    "\n",
    "    edges['lisCel'] = reached_cells\n",
    "    edges['ldsAre'] = areas\n",
    "\n",
    "    # Link together \n",
    "    e_to_link = ['sdsAre', 'sisBpM', 'misCel', 'mdsAre', 'lisCel', 'ldsAre']\n",
    "    n_to_link = 'sddAre'\n",
    "\n",
    "    cells = cells.merge(nodes[['nodeID', 'sddAre']], on='nodeID', how='left')\n",
    "\n",
    "    l = []\n",
    "    for keys, values in zip(cells.edgeID_keys, cells.edgeID_values):\n",
    "        l.append((edges.iloc[keys][e_to_link].multiply(values, axis='rows')).sum(axis=0))  # weighted by the proportion\n",
    "    cells[e_to_link] = pd.DataFrame(l, index=cells.index)\n",
    "    \n",
    "    # Reached neighbors and area on 3 topological steps on tessellation\n",
    "    cells['keep'] = True\n",
    "    \n",
    "    # add neighbouring cells from other chunks\n",
    "    cross_chunk_cells = []\n",
    "    \n",
    "    for chunk, inds in cross_chunk.loc[chunk_id].indices.iteritems():\n",
    "        add_cells = geopandas.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/cells/cells_{chunk}.pq\").iloc[inds]\n",
    "        add_cells['keep'] = False\n",
    "        cross_chunk_cells.append(add_cells)\n",
    "    \n",
    "    df = cells.append(pd.concat(cross_chunk_cells, ignore_index=True), ignore_index=True)\n",
    "    w3 = libpysal.weights.WSP(scipy.sparse.load_npz(f\"../../urbangrammar_samba/spatial_signatures/weights/w3_{chunk_id}.npz\")).to_W()\n",
    "    \n",
    "    # Reached cells in 3 topological steps\n",
    "    df['ltcRea'] = [w3.cardinalities[i] for i in range(len(df))]\n",
    "    \n",
    "    # Reached area in 3 topological steps\n",
    "    df['ltcAre'] = [df.sdcAre.iloc[w3.neighbors[i]].sum() for i in range(len(df))]\n",
    "    \n",
    "    # Save\n",
    "    df[df['keep']].drop(columns=['keep']).to_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/cells/cells_{chunk_id}.pq\")\n",
    "    \n",
    "    return f\"Chunk {chunk_id} processed sucessfully in {time() - s} seconds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "inputs = iter(range(103))\n",
    "futures = [client.submit(measure, next(inputs)) for i in range(workers)]\n",
    "ac = as_completed(futures)\n",
    "for finished_future in ac:\n",
    "    # submit new future \n",
    "    try:\n",
    "        new_future = client.submit(measure, next(inputs))\n",
    "        ac.add(new_future)\n",
    "    except StopIteration:\n",
    "        pass\n",
    "    print(finished_future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, all primary morphometric characters are measured and stored in a chunked parquet.\n",
    "\n",
    "## Convolution\n",
    "\n",
    "Morphometric variables are an input of cluster analysis, which should result in delineation of spatial signatures. However, primary morphometric characters can't be used directly. We have to understand them in context. For that reason, we introduce a convolution step. Each of the characters above will be expressed as first, second (median) and third quartile within 3 topological steps on enclosed tessellation. Resulting convolutional data will be then used as an input of cluster analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate weights of 10th order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_chunk = pd.read_parquet('../../urbangrammar_samba/spatial_signatures/cross-chunk_indices_10.pq')\n",
    "\n",
    "def generate_w(chunk_id):\n",
    "    s = time()\n",
    "    # load cells of a chunk\n",
    "    cells = geopandas.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/cells/cells_{chunk_id}.pq\")\n",
    "    \n",
    "    # add neighbouring cells from other chunks\n",
    "    cross_chunk_cells = []\n",
    "    \n",
    "    for chunk, inds in cross_chunk.loc[chunk_id].indices.iteritems():\n",
    "        add_cells = geopandas.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/cells/cells_{chunk}.pq\").iloc[inds]\n",
    "        cross_chunk_cells.append(add_cells)\n",
    "    \n",
    "    df = cells.append(pd.concat(cross_chunk_cells, ignore_index=True), ignore_index=True)\n",
    "\n",
    "    w = libpysal.weights.Queen.from_dataframe(df, geom_col='tessellation', silence_warnings=True)\n",
    "    w10 = momepy.sw_high(k=10, weights=w)\n",
    "    \n",
    "    scipy.sparse.save_npz(f\"../../urbangrammar_samba/spatial_signatures/weights/w10_queen_{chunk_id}.npz\", w.sparse)\n",
    "    scipy.sparse.save_npz(f\"../../urbangrammar_samba/spatial_signatures/weights/w10_10_{chunk_id}.npz\", w10.sparse)\n",
    "    \n",
    "    return f\"Chunk {chunk_id} processed sucessfully in {time() - s} seconds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea9a2e7fcd9c4d84b09821b9511eb0cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 processed sucessfully in 316.37095499038696 seconds.\n",
      "Chunk 1 processed sucessfully in 491.681759595871 seconds.\n",
      "Chunk 2 processed sucessfully in 431.6562063694 seconds.\n",
      "Chunk 3 processed sucessfully in 415.92933177948 seconds.\n",
      "Chunk 4 processed sucessfully in 703.5903306007385 seconds.\n",
      "Chunk 5 processed sucessfully in 622.5808844566345 seconds.\n",
      "Chunk 6 processed sucessfully in 972.744900226593 seconds.\n",
      "Chunk 7 processed sucessfully in 582.5596327781677 seconds.\n",
      "Chunk 8 processed sucessfully in 649.1616985797882 seconds.\n",
      "Chunk 9 processed sucessfully in 489.80875730514526 seconds.\n",
      "Chunk 10 processed sucessfully in 440.14411187171936 seconds.\n",
      "Chunk 11 processed sucessfully in 410.46586871147156 seconds.\n",
      "Chunk 12 processed sucessfully in 723.2832217216492 seconds.\n",
      "Chunk 13 processed sucessfully in 523.529534816742 seconds.\n",
      "Chunk 14 processed sucessfully in 608.9691398143768 seconds.\n",
      "Chunk 15 processed sucessfully in 431.5302448272705 seconds.\n",
      "Chunk 16 processed sucessfully in 359.5892493724823 seconds.\n",
      "Chunk 17 processed sucessfully in 356.1692316532135 seconds.\n",
      "Chunk 18 processed sucessfully in 363.9447720050812 seconds.\n",
      "Chunk 19 processed sucessfully in 422.09540605545044 seconds.\n",
      "Chunk 20 processed sucessfully in 440.3369314670563 seconds.\n",
      "Chunk 21 processed sucessfully in 520.4324748516083 seconds.\n",
      "Chunk 22 processed sucessfully in 433.4581091403961 seconds.\n",
      "Chunk 23 processed sucessfully in 473.68613290786743 seconds.\n",
      "Chunk 24 processed sucessfully in 514.4364762306213 seconds.\n",
      "Chunk 25 processed sucessfully in 384.3659420013428 seconds.\n",
      "Chunk 26 processed sucessfully in 561.0015258789062 seconds.\n",
      "Chunk 27 processed sucessfully in 522.7717218399048 seconds.\n",
      "Chunk 28 processed sucessfully in 405.17521572113037 seconds.\n",
      "Chunk 29 processed sucessfully in 400.2644863128662 seconds.\n",
      "Chunk 30 processed sucessfully in 683.7991404533386 seconds.\n",
      "Chunk 31 processed sucessfully in 377.3410851955414 seconds.\n",
      "Chunk 32 processed sucessfully in 709.951803445816 seconds.\n",
      "Chunk 33 processed sucessfully in 392.99678206443787 seconds.\n",
      "Chunk 34 processed sucessfully in 369.2902057170868 seconds.\n",
      "Chunk 35 processed sucessfully in 467.20433378219604 seconds.\n",
      "Chunk 36 processed sucessfully in 311.1378049850464 seconds.\n",
      "Chunk 38 processed sucessfully in 728.8909161090851 seconds.\n",
      "Chunk 39 processed sucessfully in 472.73110246658325 seconds.\n",
      "Chunk 40 processed sucessfully in 857.3202881813049 seconds.\n",
      "Chunk 41 processed sucessfully in 462.12418603897095 seconds.\n",
      "Chunk 42 processed sucessfully in 469.5953435897827 seconds.\n",
      "Chunk 43 processed sucessfully in 433.78106570243835 seconds.\n",
      "Chunk 44 processed sucessfully in 403.88263988494873 seconds.\n",
      "Chunk 45 processed sucessfully in 481.71834993362427 seconds.\n",
      "Chunk 46 processed sucessfully in 556.1396548748016 seconds.\n"
     ]
    }
   ],
   "source": [
    "# I am afraid that we would run out of memory if we did this in parallel\n",
    "for i in tqdm(range(103), total=103):\n",
    "    print(generate_w(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_distance_w(chunk_id):\n",
    "    s = time()\n",
    "    # load cells of a chunk\n",
    "    points = geopandas.read_parquet(f'../../urbangrammar_samba/spatial_signatures/inscribed_circle/circle_{chunk_id}.pq')\n",
    "    \n",
    "    # add neighbouring cells from other chunks\n",
    "    cross_chunk_cells = []\n",
    "    \n",
    "    for chunk, inds in cross_chunk.loc[chunk_id].indices.iteritems():\n",
    "        add_cells = geopandas.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/inscribed_circle/circle_{chunk}.pq\").iloc[inds]\n",
    "        cross_chunk_cells.append(add_cells)\n",
    "    \n",
    "    df = points.append(pd.concat(cross_chunk_cells, ignore_index=True), ignore_index=True)\n",
    "\n",
    "    w = libpysal.weights.WSP(scipy.sparse.load_npz(f\"../../urbangrammar_samba/spatial_signatures/weights/w10_10_{chunk_id}.npz\")).to_W()\n",
    "\n",
    "    for i, (radius, geom) in enumerate(points[['radius', 'geometry']].itertuples(index=False)):\n",
    "        neighbours = w.neighbors[i]\n",
    "        vicinity = df.iloc[neighbours]\n",
    "        distance = vicinity.distance(geom).to_list()\n",
    "       \n",
    "        distance.append(radius)\n",
    "        w.neighbors[i].append(i)\n",
    "        w.weights[i] = distance\n",
    "    \n",
    "    scipy.sparse.save_npz(f\"../../urbangrammar_samba/spatial_signatures/weights/w10_10_distance_circles_{chunk_id}.npz\", w.sparse)\n",
    "    \n",
    "    return f\"Chunk {chunk_id} processed sucessfully in {time() - s} seconds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be445fa59de245aa8bb91d803a6284c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 processed sucessfully in 198.621652841568 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 17 disconnected components.\n",
      " There are 7 islands with ids: 129987, 133329, 154196, 164082, 173764, 174296, 178630.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1 processed sucessfully in 246.69801259040833 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 8 disconnected components.\n",
      " There are 2 islands with ids: 128344, 155326.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 2 processed sucessfully in 224.87160181999207 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 11 disconnected components.\n",
      " There are 5 islands with ids: 111576, 115679, 130190, 135459, 144200.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 3 processed sucessfully in 211.8876757621765 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 9 disconnected components.\n",
      " There are 3 islands with ids: 197896, 219887, 225719.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 4 processed sucessfully in 341.19576501846313 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 216 disconnected components.\n",
      " There are 162 islands with ids: 234211, 234213, 234707, 234708, 234725, 234783, 234785, 234789, 234796, 234801, 234803, 234810, 234870, 234958, 234965, 234969, 234983, 235019, 235024, 235042, 235045, 235047, 235055, 235058, 235059, 235062, 235063, 235066, 235067, 235071, 235076, 235077, 235080, 235082, 235089, 235094, 235101, 235104, 235106, 235107, 235111, 235120, 235121, 235138, 235142, 235143, 235146, 235148, 235149, 235150, 235152, 235155, 235157, 235158, 235167, 235168, 235169, 235175, 235179, 235188, 235190, 235214, 235216, 235217, 235220, 235221, 235226, 235229, 235230, 235235, 235242, 235250, 235255, 235271, 235285, 235287, 235315, 235321, 235338, 235348, 235362, 235388, 235393, 235399, 235409, 235413, 235425, 235441, 235442, 235443, 235444, 235445, 235446, 235447, 235453, 235455, 235461, 235462, 235464, 235465, 235471, 235472, 235490, 235491, 235493, 235501, 235502, 235503, 235506, 235511, 235516, 235517, 235537, 235541, 235557, 235559, 235566, 235567, 235572, 235593, 235597, 235603, 235609, 235615, 235658, 235672, 235679, 235683, 235692, 235709, 235714, 235721, 235731, 235738, 235743, 235744, 235751, 235752, 235753, 235754, 235761, 235762, 235766, 235767, 235768, 235786, 235787, 235789, 235790, 235792, 235793, 235794, 235795, 235798, 235800, 235809, 235839, 235840, 235854, 235873, 235874, 235875.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 5 processed sucessfully in 396.4719624519348 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 17 disconnected components.\n",
      " There are 7 islands with ids: 261627, 261628, 265375, 270336, 271107, 273452, 325283.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 6 processed sucessfully in 504.87192392349243 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 11 disconnected components.\n",
      " There are 6 islands with ids: 146067, 157019, 178978, 179921, 196088, 201462.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 7 processed sucessfully in 282.91939425468445 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 9 disconnected components.\n",
      " There are 5 islands with ids: 149961, 156543, 186429, 210738, 211707.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 8 processed sucessfully in 308.07550287246704 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 11 disconnected components.\n",
      " There are 6 islands with ids: 131784, 144438, 145139, 148759, 178003, 182317.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 9 processed sucessfully in 250.7905731201172 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 6 disconnected components.\n",
      " There is 1 island with id: 163162.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 10 processed sucessfully in 221.5810604095459 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 7 disconnected components.\n",
      " There are 2 islands with ids: 119450, 134297.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 11 processed sucessfully in 210.34161281585693 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 8 disconnected components.\n",
      " There are 4 islands with ids: 209165, 209170, 213280, 230710.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 12 processed sucessfully in 391.3736517429352 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 14 disconnected components.\n",
      " There are 8 islands with ids: 141991, 141993, 170904, 171323, 172109, 172779, 173220, 189518.\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# I am afraid that we would run out of memory if we did this in parallel\n",
    "for i in tqdm(range(103), total=103):\n",
    "    print(generate_distance_w(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = ['sdbAre', 'sdbPer', 'sdbCoA', 'ssbCCo', 'ssbCor', 'ssbSqu', 'ssbERI', 'ssbElo', \n",
    "             'ssbCCM', 'ssbCCD', 'stbOri', 'sdcLAL', 'sdcAre', 'sscCCo', 'sscERI', 'stcOri', \n",
    "             'sicCAR', 'stbCeA', 'mtbAli', 'mtbNDi', 'mtcWNe', 'mdcAre', 'ltcWRE', 'ltbIBD', \n",
    "             'sdsSPW', 'sdsSWD', 'sdsSPO', 'sdsLen', 'sssLin', 'ldsMSL', 'mtdDeg', 'lcdMes', \n",
    "             'linP3W', 'linP4W', 'linPDE', 'lcnClo', 'ldsCDL', 'xcnSCl', 'mtdMDi', 'lddNDe', \n",
    "             'linWID', 'stbSAl', 'sddAre', 'sdsAre', 'sisBpM', 'misCel', 'mdsAre', 'lisCel', \n",
    "             'ldsAre', 'ltcRea', 'ltcAre', 'ldeAre', 'ldePer', 'lseCCo', 'lseERI', 'lseCWA', \n",
    "             'lteOri', 'lteWNB', 'lieWCe'\n",
    "            ]\n",
    "\n",
    "def convolute(chunk_id):\n",
    "   \n",
    "    s = time()\n",
    "    cells = geopandas.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/cells/cells_{chunk_id}.pq\")\n",
    "    cells['keep'] = True\n",
    "    # add neighbouring cells from other chunks\n",
    "    cross_chunk_cells = []\n",
    "\n",
    "    for chunk, inds in cross_chunk.loc[chunk_id].indices.iteritems():\n",
    "        add_cells = geopandas.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/cells/cells_{chunk}.pq\").iloc[inds]\n",
    "        add_cells['keep'] = False\n",
    "        cross_chunk_cells.append(add_cells)\n",
    "\n",
    "    df = cells.append(pd.concat(cross_chunk_cells, ignore_index=True), ignore_index=True)\n",
    "\n",
    "    # read W\n",
    "    W = libpysal.weights.WSP(scipy.sparse.load_npz(f\"../../urbangrammar_samba/spatial_signatures/weights/w10_10_distance_circles_{chunk_id}.npz\")).to_W()\n",
    " \n",
    "    # prepare dictionary to store results\n",
    "    convolutions = {}\n",
    "    for c in characters:\n",
    "        convolutions[c] = []\n",
    "        \n",
    "    # measure convolutions\n",
    "    for i in range(len(df)):\n",
    "        neighbours = W.neighbors[i]\n",
    "        vicinity = df.iloc[neighbours]\n",
    "        distance = W.weights[i]\n",
    "        distance_decay = 1 / np.array(distance)\n",
    "        \n",
    "        for c in characters:\n",
    "            values = vicinity[c].values\n",
    "            sorter = np.argsort(values)\n",
    "            values = values[sorter]\n",
    "            nan_mask = np.isnan(values)\n",
    "            if nan_mask.all():\n",
    "                convolutions[c].append(np.array([np.nan] * 3))\n",
    "            else:\n",
    "                sample_weight = distance_decay[sorter][~nan_mask]\n",
    "                weighted_quantiles = np.cumsum(sample_weight) - 0.5 * sample_weight\n",
    "                weighted_quantiles /= np.sum(sample_weight)\n",
    "                interpolate = np.interp([.25, .5, .75], weighted_quantiles, values[~nan_mask])\n",
    "                convolutions[c].append(interpolate)\n",
    "    \n",
    "    # save convolutions to parquet file\n",
    "    conv = pd.DataFrame(convolutions, index=df.index)\n",
    "    exploded = pd.concat([pd.DataFrame(conv[c].to_list(), columns=[c + '_q1', c + '_q2',c + '_q3']) for c in characters], axis=1)\n",
    "    exploded[df.keep].to_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/convolutions/conv_{chunk_id}.pq\")\n",
    "        \n",
    "    return f\"Chunk {chunk_id} processed sucessfully in {time() - s} seconds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83406a40c0334bac9efbdf47e7f16565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 processed sucessfully in 796.7945809364319 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 17 disconnected components.\n",
      " There are 7 islands with ids: 129987, 133329, 154196, 164082, 173764, 174296, 178630.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1 processed sucessfully in 1142.5891633033752 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 8 disconnected components.\n",
      " There are 2 islands with ids: 128344, 155326.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 2 processed sucessfully in 1059.1576709747314 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 11 disconnected components.\n",
      " There are 5 islands with ids: 111576, 115679, 130190, 135459, 144200.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 3 processed sucessfully in 1004.6432518959045 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 9 disconnected components.\n",
      " There are 3 islands with ids: 197896, 219887, 225719.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 4 processed sucessfully in 1590.0092322826385 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 216 disconnected components.\n",
      " There are 162 islands with ids: 234211, 234213, 234707, 234708, 234725, 234783, 234785, 234789, 234796, 234801, 234803, 234810, 234870, 234958, 234965, 234969, 234983, 235019, 235024, 235042, 235045, 235047, 235055, 235058, 235059, 235062, 235063, 235066, 235067, 235071, 235076, 235077, 235080, 235082, 235089, 235094, 235101, 235104, 235106, 235107, 235111, 235120, 235121, 235138, 235142, 235143, 235146, 235148, 235149, 235150, 235152, 235155, 235157, 235158, 235167, 235168, 235169, 235175, 235179, 235188, 235190, 235214, 235216, 235217, 235220, 235221, 235226, 235229, 235230, 235235, 235242, 235250, 235255, 235271, 235285, 235287, 235315, 235321, 235338, 235348, 235362, 235388, 235393, 235399, 235409, 235413, 235425, 235441, 235442, 235443, 235444, 235445, 235446, 235447, 235453, 235455, 235461, 235462, 235464, 235465, 235471, 235472, 235490, 235491, 235493, 235501, 235502, 235503, 235506, 235511, 235516, 235517, 235537, 235541, 235557, 235559, 235566, 235567, 235572, 235593, 235597, 235603, 235609, 235615, 235658, 235672, 235679, 235683, 235692, 235709, 235714, 235721, 235731, 235738, 235743, 235744, 235751, 235752, 235753, 235754, 235761, 235762, 235766, 235767, 235768, 235786, 235787, 235789, 235790, 235792, 235793, 235794, 235795, 235798, 235800, 235809, 235839, 235840, 235854, 235873, 235874, 235875.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 5 processed sucessfully in 1508.7903985977173 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 17 disconnected components.\n",
      " There are 7 islands with ids: 261627, 261628, 265375, 270336, 271107, 273452, 325283.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 6 processed sucessfully in 2171.5777893066406 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 11 disconnected components.\n",
      " There are 6 islands with ids: 146067, 157019, 178978, 179921, 196088, 201462.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 7 processed sucessfully in 1352.2515318393707 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 9 disconnected components.\n",
      " There are 5 islands with ids: 149961, 156543, 186429, 210738, 211707.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 8 processed sucessfully in 1437.4338595867157 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 11 disconnected components.\n",
      " There are 6 islands with ids: 131784, 144438, 145139, 148759, 178003, 182317.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 9 processed sucessfully in 1178.344761133194 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 6 disconnected components.\n",
      " There is 1 island with id: 163162.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 10 processed sucessfully in 1038.3264207839966 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 7 disconnected components.\n",
      " There are 2 islands with ids: 119450, 134297.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 11 processed sucessfully in 966.2513835430145 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 8 disconnected components.\n",
      " There are 4 islands with ids: 209165, 209170, 213280, 230710.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 12 processed sucessfully in 1657.830234527588 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 14 disconnected components.\n",
      " There are 8 islands with ids: 141991, 141993, 170904, 171323, 172109, 172779, 173220, 189518.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 13 processed sucessfully in 1231.476334810257 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 26 disconnected components.\n",
      " There are 10 islands with ids: 149934, 149961, 150595, 151291, 173515, 185651, 194258, 195351, 196488, 204577.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 14 processed sucessfully in 1331.538037776947 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 13 disconnected components.\n",
      " There are 4 islands with ids: 104333, 106066, 135397, 155471.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 15 processed sucessfully in 1011.0636105537415 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 11 disconnected components.\n",
      " There are 4 islands with ids: 120266, 129179, 135636, 150457.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 16 processed sucessfully in 917.4892508983612 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 5 disconnected components.\n",
      " There are 3 islands with ids: 119002, 124069, 125308.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 17 processed sucessfully in 921.7690391540527 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 7 disconnected components.\n",
      " There are 3 islands with ids: 105404, 117589, 128371.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 18 processed sucessfully in 918.0861921310425 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 5 disconnected components.\n",
      " There are 2 islands with ids: 160236, 167180.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 19 processed sucessfully in 1056.5993974208832 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 16 disconnected components.\n",
      " There are 5 islands with ids: 108513, 108636, 108941, 141461, 158970.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 20 processed sucessfully in 1074.1219747066498 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 21 disconnected components.\n",
      " There are 5 islands with ids: 117072, 120055, 120436, 139715, 165715.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 21 processed sucessfully in 1145.8573162555695 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 8 disconnected components.\n",
      " There are 2 islands with ids: 113667, 137661.\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# I am afraid that we would run out of memory if we did this in parallel\n",
    "for i in tqdm(range(103), total=103):\n",
    "    print(convolute(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
