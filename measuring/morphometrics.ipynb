{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Urban morphometrics\n",
    "\n",
    "Morpohometric assessment measure wide range of characters of urban form to derive a complex description of built-up patterns composed of enclosed tessellation, buildings and street network.\n",
    "\n",
    "All algorithms used within this notebook are part of `momepy` Python toolkit and can be used from there. We have extracted them from `momepy`, adapted for `dask` and `pygeos` and used in raw form tailored directly to our use case. The algorithms which were enhanced are pushed back to momepy and will be part of `momepy` 0.4.0.\n",
    "\n",
    "All steps within this notebook are parallelised using `dask`. The first part, which measures aspects of individual elements (does not require to know the context) uses pre-release of `dask-geopandas`. The rest uses `dask` to manage parallel iteration over geo-chunks with single-core algorithms. \n",
    "\n",
    "Some functions are imported from a `momepy_utils.py` file stored wihtin this directory. Those are either helper functions taken directly from momepy or their enhanced versions, all which will be included in the next release of momepy.\n",
    "\n",
    "## Individual elements\n",
    "\n",
    "Note: Requires dask-geopandas and current master of geopandas to support dask version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+git://github.com/jsignell/dask-geopandas.git\n",
    "# !pip install git+git://github.com/geopandas/geopandas.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "from time import time\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import dask_geopandas as dask_geopandas\n",
    "import geopandas\n",
    "import libpysal\n",
    "import momepy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pygeos\n",
    "import scipy\n",
    "from dask.distributed import Client, LocalCluster, as_completed\n",
    "from libpysal.weights import Queen\n",
    "from momepy_utils import (\n",
    "    _circle_radius,\n",
    "    centroid_corner,\n",
    "    elongation,\n",
    "    get_corners,\n",
    "    get_edge_ratios,\n",
    "    get_nodes,\n",
    "    solar_orientation_poly,\n",
    "    squareness,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using a single machine wihtin this notebook with 14 cores, so we start local dask cluster with 14 workers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(LocalCluster(n_workers=14))\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dask-geopandas` is still under development and raises few warnigns at the moment, all which can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore', message='.*initial implementation of Parquet.*')\n",
    "warnings.filterwarnings('ignore', message='.*Assigning CRS to a GeoDataFrame without a geometry*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring buildings and enclosed cells\n",
    "\n",
    "In the first step, we iterate over geo-chunks, merge enclosed tessellation and buildings to a single `geopandas.GeoDataFrame` and convert it to `dask.GeoDataFrame`. The rest of the code is mostly an extraction from momepy source code adapted for dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk_id in tqdm(range(103), total=103):\n",
    "    \n",
    "    # Load data and merge them together\n",
    "    blg = geopandas.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/buildings/blg_{chunk_id}.pq\")\n",
    "    tess = geopandas.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/tessellation/tess_{chunk_id}.pq\")\n",
    "    \n",
    "    blg = blg.rename_geometry('buildings')\n",
    "    tess = tess.rename_geometry('tessellation')\n",
    "\n",
    "    df = tess.merge(blg, on='uID', how='left')\n",
    "    \n",
    "    # Convert to dask.GeoDataFrame\n",
    "    ddf = dask_geopandas.from_geopandas(df, npartitions=14)\n",
    "    \n",
    "    ## Measure morphometric characters\n",
    "    # Building area\n",
    "    ddf['sdbAre'] = ddf.buildings.area\n",
    "    \n",
    "    # Building perimeter\n",
    "    ddf['sdbPer'] = ddf.buildings.length\n",
    "    \n",
    "    # Courtyard area\n",
    "    exterior_area = ddf.buildings.map_partitions(lambda series: pygeos.area(pygeos.polygons(series.exterior.values.data)), meta='float')\n",
    "    ddf['sdbCoA'] = exterior_area - ddf['sdbAre']\n",
    "\n",
    "    # Circular compactness\n",
    "    hull = ddf.buildings.convex_hull.exterior\n",
    "\n",
    "    radius = hull.apply(lambda g: _circle_radius(list(g.coords)) if g is not None else None, meta='float')\n",
    "    ddf['ssbCCo'] = ddf['sdbAre'] / (np.pi * radius ** 2)\n",
    "\n",
    "    # Corners\n",
    "    ddf['ssbCor'] = ddf.buildings.apply(lambda g: get_corners(g), meta='float')\n",
    "\n",
    "    # Squareness\n",
    "    ddf['ssbSqu'] = ddf.buildings.apply(lambda g: squareness(g), meta='float')\n",
    "    \n",
    "    # Equivalent rectangular index\n",
    "    bbox = ddf.buildings.apply(lambda g: g.minimum_rotated_rectangle if g is not None else None, meta=geopandas.GeoSeries())\n",
    "    ddf['ssbERI'] = (ddf['sdbAre'] / bbox.area).pow(1./2) * (bbox.length / ddf['sdbPer'])\n",
    "\n",
    "    # Elongation\n",
    "    ddf['ssbElo'] = bbox.map_partitions(lambda s: elongation(s), meta='float')\n",
    "    \n",
    "    # Centroid corner mean distance and deviation\n",
    "    def _centroid_corner(series):\n",
    "        ccd = series.apply(lambda g: centroid_corner(g))\n",
    "        return pd.DataFrame(ccd.to_list(), index=series.index)\n",
    "\n",
    "    \n",
    "    ddf[['ssbCCM', 'ssbCCD']] = ddf.buildings.map_partitions(_centroid_corner, meta=pd.DataFrame({0: [0.1], 1: [1.1]}))\n",
    "    \n",
    "    # Solar orientation\n",
    "    ddf['stbOri'] = bbox.apply(lambda g: solar_orientation_poly(g), meta='float')\n",
    "    \n",
    "    # Tessellation longest axis length\n",
    "    hull = ddf.tessellation.convex_hull.exterior\n",
    "\n",
    "    ddf['sdcLAL'] = hull.apply(lambda g: _circle_radius(list(g.coords)), meta='float') * 2\n",
    "    \n",
    "    # Tessellation area\n",
    "    ddf['sdcAre'] = ddf.tessellation.area\n",
    "    \n",
    "    # Circular compactness\n",
    "    radius = hull.apply(lambda g: _circle_radius(list(g.coords)), meta='float')\n",
    "    ddf['sscCCo'] = ddf['sdcAre'] / (np.pi * radius ** 2)\n",
    "    \n",
    "    # Equivalent rectangular index\n",
    "    bbox = ddf.tessellation.apply(lambda g: g.minimum_rotated_rectangle, meta=geopandas.GeoSeries())\n",
    "    ddf['sscERI'] = (ddf['sdcAre'] / bbox.area).pow(1./2) * (bbox.length / ddf.tessellation.length)\n",
    "    \n",
    "    # Solar orientation\n",
    "    ddf['stcOri'] = bbox.apply(lambda g: solar_orientation_poly(g), meta='float')\n",
    "    \n",
    "    # Covered area ratio\n",
    "    ddf['sicCAR'] = ddf['sdbAre'] / ddf['sdcAre']\n",
    "    \n",
    "    # Building-cell alignment\n",
    "    ddf['stbCeA'] = (ddf['stbOri'] - ddf['stcOri']).abs()\n",
    "    \n",
    "    # Compute all characters using dask\n",
    "    df = ddf.compute()\n",
    "    \n",
    "    # Save to parquet file\n",
    "    df.to_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/cells/cells_{chunk_id}.pq\")\n",
    "    client.restart()\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring enclosures\n",
    "\n",
    "All enclosures are loaded as a single dask.GeoDataFrame and measured at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Load data\n",
    "encl = dask_geopandas.read_parquet(\"../../urbangrammar_samba/spatial_signatures/enclosures/encl_*.pq\")\n",
    "\n",
    "# Area\n",
    "encl['ldeAre'] = encl.geometry.area\n",
    "\n",
    "# Perimeter\n",
    "encl['ldePer'] = encl.geometry.length\n",
    "\n",
    "# Circular compacntess\n",
    "hull = encl.geometry.convex_hull.exterior\n",
    "\n",
    "radius = hull.apply(lambda g: _circle_radius(list(g.coords)) if g is not None else None, meta='float')\n",
    "encl['lseCCo'] = encl['ldeAre'] / (np.pi * radius ** 2)\n",
    "\n",
    "# Equivalent rectangular index\n",
    "bbox = encl.geometry.apply(lambda g: g.minimum_rotated_rectangle if g is not None else None, meta=geopandas.GeoSeries())\n",
    "encl['lseERI'] = (encl['ldeAre'] / bbox.area).pow(1./2) * (bbox.length / encl['ldePer'])\n",
    "\n",
    "# Compactness-weighted axis\n",
    "longest_axis = hull.apply(lambda g: _circle_radius(list(g.coords)), meta='float') * 2\n",
    "encl['lseCWA'] = longest_axis * ((4 / np.pi) - (16 * encl['ldeAre']) / ((encl['ldePer']) ** 2))\n",
    "\n",
    "# Solar orientation\n",
    "encl['lteOri'] = bbox.apply(lambda g: solar_orientation_poly(g), meta='float')\n",
    "\n",
    "# Compute data and return geopandas.GeoDataFrame\n",
    "encl_df = encl.compute()\n",
    "\n",
    "# Weighted number of neighbors\n",
    "inp, res = encl_df.sindex.query_bulk(encl_df.geometry, predicate='intersects')\n",
    "indices, counts = np.unique(inp, return_counts=True)\n",
    "encl_df['neighbors'] = counts - 1\n",
    "encl_df['lteWNB'] = encl_df['neighbors'] / encl_df['ldePer']\n",
    "\n",
    "# Load complete enclosed tessellation as a dask.GeoDataFrame\n",
    "tess = dd.read_parquet(\"../../urbangrammar_samba/spatial_signatures/tessellation/tess_*.pq\")\n",
    "\n",
    "# Measure weighted cells within enclosure\n",
    "encl_counts = tess.groupby('enclosureID').count().compute()\n",
    "merged = encl_df[['enclosureID', 'ldeAre']].merge(encl_counts[['geometry']], how='left', on='enclosureID')\n",
    "encl_df['lieWCe'] = merged['geometry'] / merged['ldeAre']\n",
    "\n",
    "# Save data to parquet\n",
    "encl_df.drop(columns='geometry').to_parquet(\"../../urbangrammar_samba/spatial_signatures/morphometrics/enclosures.pq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now close dask client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate spatial weights (W)\n",
    "\n",
    "Subsequent steps will require understanding of the context of each tessellation cell in a form of spatial weights matrices (Queen contiguity and Queen contiguty of inclusive 3rd order). We generate them beforehand and store as `npz` files representing sparse matrix.\n",
    "\n",
    "Each geo-chunk is loaded together with relevant cross-chunk tessellation cells (to avoid edge effect). We use dask to parallelise the iteration. Number of workers is smaller now to ensure enough memory for each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = 8\n",
    "client = Client(LocalCluster(n_workers=workers, threads_per_worker=1))\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to specify a function doing the processing itself, where the only attribure is the `chunk_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_w(chunk_id):\n",
    "    # load cells of a chunk\n",
    "    cells = geopandas.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/cells/cells_{chunk_id}.pq\")\n",
    "    \n",
    "    # add neighbouring cells from other chunks\n",
    "    cross_chunk_cells = []\n",
    "    \n",
    "    for chunk, inds in cross_chunk.loc[chunk_id].indices.iteritems():\n",
    "        add_cells = geopandas.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/cells/cells_{chunk}.pq\").iloc[inds]\n",
    "        cross_chunk_cells.append(add_cells)\n",
    "    \n",
    "    df = cells.append(pd.concat(cross_chunk_cells, ignore_index=True), ignore_index=True)\n",
    "\n",
    "    w = libpysal.weights.Queen.from_dataframe(df, geom_col='tessellation')\n",
    "    w3 = momepy.sw_high(k=3, weights=w)\n",
    "    \n",
    "    scipy.sparse.save_npz(f\"../../urbangrammar_samba/spatial_signatures/weights/w_{chunk_id}.npz\", w.sparse)\n",
    "    scipy.sparse.save_npz(f\"../../urbangrammar_samba/spatial_signatures/weights/w3_{chunk_id}.npz\", w3.sparse)\n",
    "    \n",
    "    return f\"Chunk {chunk_id} processed sucessfully.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use dask to iterate over all 103 chunks. The following script sends first 8 chunks to dask together and then submits a new chunk as soon as any of previous finishes (courtesy of Matthew Rocklin). That way we process only 8 chunks at once ensuring that we the cluster will not run out of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "inputs = iter(range(103))\n",
    "futures = [client.submit(generate_w, next(inputs)) for i in range(workers)]\n",
    "ac = as_completed(futures)\n",
    "for finished_future in ac:\n",
    "    # submit new future \n",
    "    try:\n",
    "        new_future = client.submit(generate_w, next(inputs))\n",
    "        ac.add(new_future)\n",
    "    except StopIteration:\n",
    "        pass\n",
    "    print(finished_future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial distribution and network analysis\n",
    "\n",
    "To measure spatial distribution of we use single-core algorithm and parallelise iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = 8\n",
    "client = Client(LocalCluster(n_workers=workers, threads_per_worker=1))\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to load street network data from PostGIS datatabase, so we establish a connection which will be used within the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_chunk = pd.read_parquet('../../urbangrammar_samba/spatial_signatures/cross-chunk_indices.pq')\n",
    "chunks = geopandas.read_parquet('../../urbangrammar_samba/spatial_signatures/local_auth_chunks.pq')\n",
    "\n",
    "user = os.environ.get('DB_USER')\n",
    "pwd = os.environ.get('DB_PWD')\n",
    "host = os.environ.get('DB_HOST')\n",
    "port = os.environ.get('DB_PORT')\n",
    "\n",
    "db_connection_url = f\"postgres+psycopg2://{user}:{pwd}@{host}:{port}/built_env\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the same function below we measure spatial distribution of elements and network-based characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(chunk_id):\n",
    "    # load cells of a chunk\n",
    "    cells = geopandas.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/cells/cells_{chunk_id}.pq\")\n",
    "    cells['keep'] = True\n",
    "    \n",
    "    # add neighbouring cells from other chunks\n",
    "    cross_chunk_cells = []\n",
    "    \n",
    "    for chunk, inds in cross_chunk.loc[chunk_id].indices.iteritems():\n",
    "        add_cells = geopandas.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/cells/cells_{chunk}.pq\").iloc[inds]\n",
    "        add_cells['keep'] = False\n",
    "        cross_chunk_cells.append(add_cells)\n",
    "    \n",
    "    df = cells.append(pd.concat(cross_chunk_cells, ignore_index=True), ignore_index=True)\n",
    "\n",
    "    # read W\n",
    "    w = libpysal.weights.WSP(scipy.sparse.load_npz(f\"../../urbangrammar_samba/spatial_signatures/weights/w_{chunk_id}.npz\")).to_W()\n",
    "    \n",
    "    # alignment\n",
    "    def alignment(x, orientation='stbOri'):\n",
    "        orientations = df[orientation].iloc[w.neighbors[x]]\n",
    "        return abs(orientations - df[orientation].iloc[x]).mean()\n",
    "    \n",
    "    df['mtbAli'] = [alignment(x) for x in range(len(df))]\n",
    "\n",
    "    # mean neighbour distance\n",
    "    def neighbor_distance(x):\n",
    "        geom = df.buildings.iloc[x]\n",
    "        if geom is None:\n",
    "            return np.nan\n",
    "        return df.buildings.iloc[w.neighbors[x]].distance(df.buildings.iloc[x]).mean()\n",
    "\n",
    "    df['mtbNDi'] = [neighbor_distance(x) for x in range(len(df))]\n",
    "    \n",
    "    # weighted neighbours\n",
    "    df['mtcWNe'] = pd.Series([w.cardinalities[x] for x in range(len(df))], index=df.index) / df.tessellation.length\n",
    "    \n",
    "    # area covered by neighbours\n",
    "    def area_covered(x, area='sdcAre'):\n",
    "        neighbours = [x]\n",
    "        neighbours += w.neighbors[x]\n",
    "\n",
    "        return df[area].iloc[neighbours].sum()\n",
    "\n",
    "    df['mdcAre'] = [area_covered(x) for x in range(len(df))]\n",
    "    \n",
    "    # read W3\n",
    "    w3 = libpysal.weights.WSP(scipy.sparse.load_npz(f\"../../urbangrammar_samba/spatial_signatures/weights/w3_{chunk_id}.npz\")).to_W()\n",
    "      \n",
    "    # weighted reached enclosures\n",
    "    def weighted_reached_enclosures(x, area='sdcAre', enclosure_id='enclosureID'):\n",
    "        neighbours = [x]\n",
    "        neighbours += w3.neighbors[x]\n",
    "\n",
    "        vicinity = df[[area, enclosure_id]].iloc[neighbours]\n",
    "\n",
    "        return vicinity[enclosure_id].unique().shape[0] / vicinity[area].sum()\n",
    "    \n",
    "    df['ltcWRE'] = [weighted_reached_enclosures(x) for x in range(len(df))]\n",
    "    \n",
    "    # mean interbuilding distance\n",
    "    # define adjacency list from lipysal\n",
    "    adj_list = w.to_adjlist(remove_symmetric=True)\n",
    "    adj_list[\"distance\"] = (\n",
    "        df.buildings.iloc[adj_list.focal]\n",
    "        .reset_index(drop=True)\n",
    "        .distance(df.buildings.iloc[adj_list.neighbor].reset_index(drop=True)).values\n",
    "    )\n",
    "    adj_list = adj_list.set_index(['focal', 'neighbor'])\n",
    "\n",
    "\n",
    "    def mean_interbuilding_distance(x):\n",
    "        neighbours = [x]\n",
    "        neighbours += w3.neighbors[x]\n",
    "        return adj_list.distance.loc[neighbours, neighbours].mean()\n",
    "\n",
    "\n",
    "    df['ltbIBD'] = [mean_interbuilding_distance(x) for x in range(len(df))]\n",
    "    \n",
    "    # Reached neighbors and area on 3 topological steps on tessellation\n",
    "    df['ltcRea'] = [w3.cardinalities[i] for i in range(len(df))]\n",
    "    df['ltcAre'] = [df.sdcAre.iloc[w3.neighbors[i]].sum() for i in range(len(df))]\n",
    "\n",
    "    # Save cells to parquet keeping only within-chunk data not the additional neighboring\n",
    "    df[df['keep']].drop(columns=['keep']).to_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/cells/cells_{chunk_id}.pq\")\n",
    "\n",
    "    # Load street network for an extended chunk area\n",
    "    chunk_area = chunks.geometry.iloc[chunk_id].buffer(5000)  # we extend the area by 5km to minimise edge effect\n",
    "    engine = create_engine(db_connection_url)\n",
    "    sql = f\"SELECT * FROM openroads_200803_topological WHERE ST_Intersects(geometry, ST_GeomFromText('{chunk_area.wkt}',27700))\"\n",
    "    streets = geopandas.read_postgis(sql, engine, geom_col='geometry')\n",
    "    \n",
    "    # Street profile (measures width, width deviation and openness)\n",
    "    sp = street_profile(streets, blg)\n",
    "    streets['sdsSPW'] = sp[0]\n",
    "    streets['sdsSWD'] = sp[1]\n",
    "    streets['sdsSPO'] = sp[2]\n",
    "    \n",
    "    # Street segment length\n",
    "    streets['sdsLen'] = streets.length\n",
    "    \n",
    "    # Street segment linearity\n",
    "    streets['sssLin'] = momepy.Linearity(streets).series\n",
    "    \n",
    "    # Convert geopadnas.GeoDataFrame to networkx.Graph for network analysis\n",
    "    G = momepy.gdf_to_nx(streets)\n",
    "    \n",
    "    # Node degree\n",
    "    G = momepy.node_degree(G)\n",
    "    \n",
    "    # Subgraph analysis (meshedness, proportion of 0, 3 and 4 way intersections, local closeness)\n",
    "    G = momepy.subgraph(\n",
    "        G,\n",
    "        radius=5,\n",
    "        meshedness=True,\n",
    "        cds_length=False,\n",
    "        mode=\"sum\",\n",
    "        degree=\"degree\",\n",
    "        length=\"mm_len\",\n",
    "        mean_node_degree=False,\n",
    "        proportion={0: True, 3: True, 4: True},\n",
    "        cyclomatic=False,\n",
    "        edge_node_ratio=False,\n",
    "        gamma=False,\n",
    "        local_closeness=True,\n",
    "        closeness_weight=\"mm_len\",\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Cul-de-sac length\n",
    "    G = momepy.cds_length(G, radius=3, name=\"ldsCDL\", verbose=False)\n",
    "    \n",
    "    # Square clustering\n",
    "    G = momepy.clustering(G, name=\"xcnSCl\")\n",
    "    \n",
    "    # Mean node distance\n",
    "    G = momepy.mean_node_dist(G, name=\"mtdMDi\", verbose=False)\n",
    "    \n",
    "    # Convert networkx.Graph back to GeoDataFrames and W (denoting relationships between nodes)\n",
    "    nodes, edges, sw = momepy.nx_to_gdf(G, spatial_weights=True)\n",
    "    \n",
    "    # Generate inclusive higher order weights\n",
    "    edges_w3 = momepy.sw_high(k=3, gdf=edges)\n",
    "    \n",
    "    # Mean segment length\n",
    "    edges[\"ldsMSL\"] = momepy.SegmentsLength(edges, spatial_weights=edges_w3, mean=True, verbose=False).series\n",
    "    \n",
    "    # Generate inclusive higher order weights\n",
    "    nodes_w5 = momepy.sw_high(k=5, weights=sw)\n",
    "    \n",
    "    # Node density\n",
    "    nodes[\"lddNDe\"] = momepy.NodeDensity(nodes, edges, nodes_w5, verbose=False).series\n",
    "    \n",
    "    # Weighter node density\n",
    "    nodes[\"linWID\"] = momepy.NodeDensity(nodes, edges, nodes_w5, weighted=True, node_degree=\"degree\", verbose=False).series\n",
    "    \n",
    "    # Save to parquets\n",
    "    edges.to_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/edges/edges_{chunk_id}.pq\")\n",
    "    nodes.to_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/nodes/nodes_{chunk_id}.pq\")\n",
    "\n",
    "\n",
    "    return f\"Chunk {chunk_id} processed sucessfully.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we use dask to iterate over all 103 chunks. The following script sends first 8 chunks to dask together and then submits a new chunk as soon as any of previous finishes. That way we process only 8 chunks at once ensuring that we the cluster will not run out of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = iter(range(103))\n",
    "futures = [client.submit(measure, next(inputs)) for i in range(workers)]\n",
    "ac = as_completed(futures)\n",
    "for finished_future in ac:\n",
    "    # submit new future \n",
    "    try:\n",
    "        new_future = client.submit(measure, next(inputs))\n",
    "        ac.add(new_future)\n",
    "    except StopIteration:\n",
    "        pass\n",
    "    print(finished_future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link elements together\n",
    "\n",
    "For the further analysis, we need to link data measured on individual elements together. We link cells to edges based on the proportion of overlap (if a cell intersects more than one edge) and nodes based on proximity (with a restriction - node has to be on linked edge). Enclosures are linked based on enclosure ID.\n",
    "\n",
    "As above, we define a single-core function and use dask to manage parallel iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link(chunk_id):\n",
    "    s = time()\n",
    "    cells = geopandas.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/cells/cells_{chunk_id}.pq\")\n",
    "    edges = geopandas.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/edges/edges_{chunk_id}.pq\")\n",
    "    nodes = geopandas.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/nodes/nodes_{chunk_id}.pq\")\n",
    "    \n",
    "    cells['edgeID'] = get_edge_ratios(cells, edges)\n",
    "    cells['nodeID'] = get_nodes(cells, nodes, edges, 'nodeID', 'edgeID', 'node_start', 'node_end')\n",
    "    \n",
    "    characters = ['sdsSPW', 'sdsSWD', 'sdsSPO', 'sdsLen', 'sssLin', 'ldsMSL']\n",
    "    l = []\n",
    "    for d in cells.edgeID:\n",
    "        l.append((edges.iloc[list(d.keys())][characters].multiply(list(d.values()), axis='rows')).sum(axis=0))\n",
    "    cells[characters] = pd.DataFrame(l, index=cells.index)\n",
    "    \n",
    "    cells = cells.merge(nodes.drop(columns=['geometry']), on='nodeID', how='left')\n",
    "    cells = cells.rename({'degree': 'mtdDeg', 'meshedness': 'lcdMes', 'proportion_3': 'linP3W', 'proportion_4': 'linP4W',\n",
    "                     'proportion_0': 'linPDE', 'local_closeness': 'lcnClo'}, axis='columns')\n",
    "    \n",
    "    cells['edgeID_keys'] = cells.edgeID.apply(lambda d: list(d.keys()))\n",
    "    cells['edgeID_values'] = cells.edgeID.apply(lambda d: list(d.values()))\n",
    "    \n",
    "    cells.drop(columns='edgeID').to_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/cells/cells_{chunk_id}.pq\")\n",
    "    \n",
    "    return f\"Chunk {chunk_id} processed sucessfully in {time() - s} seconds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = 14\n",
    "client = Client(LocalCluster(n_workers=workers, threads_per_worker=1))\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "inputs = iter(range(103))\n",
    "futures = [client.submit(link, next(inputs)) for i in range(workers)]\n",
    "ac = as_completed(futures)\n",
    "for finished_future in ac:\n",
    "    # submit new future \n",
    "    try:\n",
    "        new_future = client.submit(link, next(inputs))\n",
    "        ac.add(new_future)\n",
    "    except StopIteration:\n",
    "        pass\n",
    "    print(finished_future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enclosures are linked via simple attribute join and since the operation is does not require any computation, it is done as a simple loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enclosures = pd.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/enclosures.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk_id in range(103):\n",
    "    s = time()\n",
    "    cells = geopandas.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/cells/cells_{chunk_id}.pq\")\n",
    "    \n",
    "    cells = cells.merge(enclosures.drop(columns=['neighbors']), on='enclosureID', how='left')\n",
    "    \n",
    "    cells.to_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/cells/cells_{chunk_id}.pq\")\n",
    "    \n",
    "    print(f\"Chunk {chunk_id} processed sucessfully in {time() - s} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inter-element characters\n",
    "\n",
    "The remaining morphometric characters are based on a relations between multiple elements. The implementation mirrors the approach above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = 8\n",
    "client = Client(LocalCluster(n_workers=workers, threads_per_worker=1))\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(chunk_id):\n",
    "    s = time()\n",
    "    # Load data\n",
    "    cells = geopandas.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/cells/cells_{chunk_id}.pq\")\n",
    "    edges = geopandas.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/edges/edges_{chunk_id}.pq\")\n",
    "    nodes = geopandas.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/nodes/nodes_{chunk_id}.pq\")\n",
    "    \n",
    "    # Street Alignment\n",
    "    edges['orient'] = momepy.Orientation(edges, verbose=False).series\n",
    "    edges['edgeID'] = range(len(edges))\n",
    "    keys = cells.edgeID_values.apply(lambda a: np.argmax(a))\n",
    "    cells['edgeID_primary'] = [inds[i] for inds, i in zip(cells.edgeID_keys, keys)]\n",
    "    cells['stbSAl'] = momepy.StreetAlignment(cells, \n",
    "                                             edges, \n",
    "                                             'stbOri', \n",
    "                                             left_network_id='edgeID_primary', \n",
    "                                             right_network_id='edgeID').series\n",
    "   \n",
    "    # Area Covered by each edge\n",
    "    vals = {x:[] for x in range(len(edges))}\n",
    "    for i, keys in enumerate(cells.edgeID_keys):\n",
    "        for k in keys:\n",
    "            vals[k].append(i)\n",
    "    area_sums = []\n",
    "    for inds in vals.values():\n",
    "        area_sums.append(cells.sdcAre.iloc[inds].sum())\n",
    "    edges['sdsAre'] = area_sums\n",
    "    \n",
    "    # Building per meter\n",
    "    bpm = []\n",
    "    for inds, l in zip(vals.values(), edges.sdsLen):\n",
    "        bpm.append(cells.buildings.iloc[inds].notna().sum() / l if len(inds) > 0 else 0)\n",
    "    edges['sisBpM'] = bpm\n",
    "    \n",
    "    # Cell area\n",
    "    nodes['sddAre'] = nodes.nodeID.apply(lambda nid: cells[cells.nodeID == nid].sdcAre.sum())\n",
    "    \n",
    "    # Area covered by neighboring edges + count of reached cells\n",
    "    edges_W = Queen.from_dataframe(edges)\n",
    "    \n",
    "    areas = []\n",
    "    reached_cells = []\n",
    "    for i in range(len(edges)):\n",
    "        neighbors = [i] + edges_W.neighbors[i]\n",
    "    #     areas\n",
    "        areas.append(edges.sdsAre.iloc[neighbors].sum())\n",
    "    #     reached cells\n",
    "        ids = []\n",
    "        for n in neighbors:\n",
    "             ids += vals[n]\n",
    "        reached_cells.append(len(set(ids)))\n",
    "\n",
    "    edges['misCel'] = reached_cells\n",
    "    edges['mdsAre'] = areas\n",
    "    \n",
    "    # Area covered by neighboring (3 steps) edges + count of reached cells\n",
    "    edges_W3 = momepy.sw_high(k=3, weights=edges_W)\n",
    "    \n",
    "    areas = []\n",
    "    reached_cells = []\n",
    "    for i in range(len(edges)):\n",
    "        neighbors = [i] + edges_W3.neighbors[i]\n",
    "    #     areas\n",
    "        areas.append(edges.sdsAre.iloc[neighbors].sum())\n",
    "    #     reached cells\n",
    "        ids = []\n",
    "        for n in neighbors:\n",
    "             ids += vals[n]\n",
    "        reached_cells.append(len(set(ids)))\n",
    "\n",
    "    edges['lisCel'] = reached_cells\n",
    "    edges['ldsAre'] = areas\n",
    "\n",
    "    # Link together \n",
    "    e_to_link = ['sdsAre', 'sisBpM', 'misCel', 'mdsAre', 'lisCel', 'ldsAre']\n",
    "    n_to_link = 'sddAre'\n",
    "\n",
    "    cells = cells.merge(nodes[['nodeID', 'sddAre']], on='nodeID', how='left')\n",
    "\n",
    "    l = []\n",
    "    for keys, values in zip(cells.edgeID_keys, cells.edgeID_values):\n",
    "        l.append((edges.iloc[keys][e_to_link].multiply(values, axis='rows')).sum(axis=0))  # weighted by the proportion\n",
    "    cells[e_to_link] = pd.DataFrame(l, index=cells.index)\n",
    "    \n",
    "    # Reached neighbors and area on 3 topological steps on tessellation\n",
    "    cells['keep'] = True\n",
    "    \n",
    "    # add neighbouring cells from other chunks\n",
    "    cross_chunk_cells = []\n",
    "    \n",
    "    for chunk, inds in cross_chunk.loc[chunk_id].indices.iteritems():\n",
    "        add_cells = geopandas.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/cells/cells_{chunk}.pq\").iloc[inds]\n",
    "        add_cells['keep'] = False\n",
    "        cross_chunk_cells.append(add_cells)\n",
    "    \n",
    "    df = cells.append(pd.concat(cross_chunk_cells, ignore_index=True), ignore_index=True)\n",
    "    w3 = libpysal.weights.WSP(scipy.sparse.load_npz(f\"../../urbangrammar_samba/spatial_signatures/weights/w3_{chunk_id}.npz\")).to_W()\n",
    "    \n",
    "    # Reached cells in 3 topological steps\n",
    "    df['ltcRea'] = [w3.cardinalities[i] for i in range(len(df))]\n",
    "    \n",
    "    # Reached area in 3 topological steps\n",
    "    df['ltcAre'] = [df.sdcAre.iloc[w3.neighbors[i]].sum() for i in range(len(df))]\n",
    "    \n",
    "    # Save\n",
    "    df[df['keep']].drop(columns=['keep']).to_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/cells/cells_{chunk_id}.pq\")\n",
    "    \n",
    "    return f\"Chunk {chunk_id} processed sucessfully in {time() - s} seconds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "inputs = iter(range(103))\n",
    "futures = [client.submit(measure, next(inputs)) for i in range(workers)]\n",
    "ac = as_completed(futures)\n",
    "for finished_future in ac:\n",
    "    # submit new future \n",
    "    try:\n",
    "        new_future = client.submit(measure, next(inputs))\n",
    "        ac.add(new_future)\n",
    "    except StopIteration:\n",
    "        pass\n",
    "    print(finished_future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, all primary morphometric characters are measured and stored in a chunked parquet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}