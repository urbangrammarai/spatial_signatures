{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "reported-retrieval",
   "metadata": {},
   "source": [
    "# Functional data\n",
    "\n",
    "This notebook links various functional layers to ET cells across GB.\n",
    "\n",
    "## Population estimates\n",
    "\n",
    "Population estimates are linked using area weighted interpolation based on building geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-saying",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tobler\n",
    "from time import time\n",
    "import xarray\n",
    "import rioxarray\n",
    "import rasterstats\n",
    "\n",
    "from dask.distributed import Client, LocalCluster, as_completed\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-proposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore', message='.*initial implementation of Parquet.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-pension",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_est = gpd.read_parquet(\"../../urbangrammar_samba/functional_data/population_estimates/gb_population_estimates.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-inspector",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = gpd.read_parquet(\"../../urbangrammar_samba/spatial_signatures/tessellation/tess_0.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-evidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin, ymin, xmax, ymax = chunk.total_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ests = tobler.area_weighted.area_interpolate(population_est.cx[xmin:xmax, ymin:ymax], chunk.set_geometry(\"buildings\"), extensive_variables=['population'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emotional-drain",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk_id in range(103):\n",
    "    s = time()\n",
    "    chunk = gpd.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/tessellation/tess_{chunk_id}.pq\", columns=[\"hindex\", \"buildings\"]).set_geometry(\"buildings\")\n",
    "    xmin, ymin, xmax, ymax = chunk.total_bounds\n",
    "    ests = tobler.area_weighted.area_interpolate(population_est.cx[xmin:xmax, ymin:ymax], chunk, extensive_variables=['population'])\n",
    "    pop = pd.DataFrame({'hindex': chunk.hindex.values, \"population\": ests.population.values})\n",
    "    pop.to_parquet(f\"../../urbangrammar_samba/spatial_signatures/functional/population/pop_{chunk_id}\")\n",
    "    print(f\"Chunk {chunk_id} processed sucessfully in {time() - s} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-routine",
   "metadata": {},
   "source": [
    "## Night lights\n",
    "\n",
    "Night lights are merged using zonal statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-transaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl = xarray.open_rasterio(\"../../urbangrammar_samba/functional_data/employment/night_lights_osgb.tif\")\n",
    "nl_clip = nl.rio.clip_box(*chunk.total_bounds)\n",
    "arr = nl_clip.values\n",
    "affine = nl_clip.rio.transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-policy",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "stats_nl = rasterstats.zonal_stats(\n",
    "    chunk.tessellation, \n",
    "    raster=arr[0],\n",
    "    affine=affine,\n",
    "    stats=['mean'],\n",
    "    all_touched=True,\n",
    "    nodata = np.nan,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-chance",
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = 8\n",
    "client = Client(LocalCluster(n_workers=workers, threads_per_worker=1))\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-wonder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _night_lights(chunk_id):\n",
    "    import rioxarray\n",
    "    \n",
    "    s = time()\n",
    "    \n",
    "    chunk = gpd.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/tessellation/tess_{chunk_id}.pq\", columns=[\"hindex\", \"tessellation\"])\n",
    "    nl = xarray.open_rasterio(\"../../urbangrammar_samba/functional_data/employment/night_lights_osgb.tif\")\n",
    "    nl_clip = nl.rio.clip_box(*chunk.total_bounds)\n",
    "    arr = nl_clip.values\n",
    "    affine = nl_clip.rio.transform()\n",
    "    stats_nl = rasterstats.zonal_stats(\n",
    "        chunk.tessellation, \n",
    "        raster=arr[0],\n",
    "        affine=affine,\n",
    "        stats=['mean'],\n",
    "        all_touched=True,\n",
    "        nodata = np.nan,\n",
    "    )\n",
    "    chunk[\"night_lights\"] = [x['mean'] for x in stats_nl]\n",
    "    chunk[[\"hindex\", \"night_lights\"]].to_parquet(f\"../../urbangrammar_samba/spatial_signatures/functional/night_lights/nl_{chunk_id}\")\n",
    "    \n",
    "    return f\"Chunk {chunk_id} processed sucessfully in {time() - s} seconds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-congo",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = iter(range(103))\n",
    "futures = [client.submit(_night_lights, next(inputs)) for i in range(workers)]\n",
    "ac = as_completed(futures)\n",
    "for finished_future in ac:\n",
    "    # submit new future \n",
    "    try:\n",
    "        new_future = client.submit(_night_lights, next(inputs))\n",
    "        ac.add(new_future)\n",
    "    except StopIteration:\n",
    "        pass\n",
    "    print(finished_future.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-johnston",
   "metadata": {},
   "source": [
    "## Worplace population by industry\n",
    "\n",
    "Worplace population is linked using area weighted interpolation based on building geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insured-average",
   "metadata": {},
   "outputs": [],
   "source": [
    "wpz = gpd.read_parquet('../../urbangrammar_samba/functional_data/employment/workplace/workplace_by_industry_gb.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-minnesota",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for chunk_id in range(103):\n",
    "    s = time()\n",
    "    chunk = gpd.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/tessellation/tess_{chunk_id}.pq\", columns=[\"hindex\", \"buildings\"]).set_geometry(\"buildings\")\n",
    "    xmin, ymin, xmax, ymax = chunk.total_bounds\n",
    "    ests = tobler.area_weighted.area_interpolate(wpz.cx[xmin:xmax, ymin:ymax], chunk, extensive_variables=wpz.columns[1:-1].to_list())\n",
    "    ests['hindex'] = chunk.hindex.values\n",
    "    ests.drop(columns=\"geometry\").to_parquet(f\"../../urbangrammar_samba/spatial_signatures/functional/workplace/pop_{chunk_id}\")\n",
    "    print(f\"Chunk {chunk_id} processed sucessfully in {time() - s} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-barrier",
   "metadata": {},
   "source": [
    "## CORINE Land cover\n",
    "\n",
    "CORINE Land cover is linked using area weighted interpolation based on tessellation geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-orange",
   "metadata": {},
   "outputs": [],
   "source": [
    "corine = gpd.read_parquet(\"../../urbangrammar_samba/functional_data/land_use/corine/corine_gb.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-physics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dask_binning(corine, cells, n_chunks=512):\n",
    "    import dask_geopandas as dgpd\n",
    "    from scipy.sparse import coo_matrix\n",
    "    \n",
    "    ids_src, ids_tgt = cells.sindex.query_bulk(corine.geometry, predicate=\"intersects\")\n",
    "    df = gpd.GeoDataFrame({'clc': corine.geometry.values[ids_src], 'tess': cells.geometry.values[ids_tgt]})\n",
    "    ddf = dgpd.from_geopandas(df, npartitions=n_chunks)\n",
    "    areas = ddf.clc.intersection(ddf.tess).area.compute()\n",
    "    table = coo_matrix(\n",
    "        (areas, (ids_src, ids_tgt),),\n",
    "        shape=(corine.shape[0], cells.shape[0]),\n",
    "        dtype=np.float32,\n",
    "    )\n",
    "\n",
    "    table = table.todok()\n",
    "\n",
    "    return table\n",
    "\n",
    "\n",
    "def _dask_area_interpolate(corine, cells, n_chunks=512, categorical_variables=None):\n",
    "    table = _dask_binning(corine, cells, n_chunks)\n",
    "    \n",
    "    if categorical_variables:\n",
    "        categorical = {}\n",
    "        for variable in categorical_variables:\n",
    "            unique = corine[variable].unique()\n",
    "            for value in unique:\n",
    "                mask = corine[variable] == value\n",
    "                categorical[f\"{variable}_{value}\"] = np.asarray(\n",
    "                    table[mask].sum(axis=0)\n",
    "                )[0]\n",
    "\n",
    "        categorical = pd.DataFrame(categorical)\n",
    "        categorical = categorical.div(cells.area, axis=\"rows\")\n",
    "    \n",
    "    return categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-cable",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk_id in range(103):\n",
    "    s = time()\n",
    "    chunk = gpd.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/tessellation/tess_{chunk_id}.pq\", columns=[\"hindex\", \"tessellation\"])\n",
    "    xmin, ymin, xmax, ymax = chunk.total_bounds\n",
    "    ests = _dask_area_interpolate(corine.cx[xmin:xmax, ymin:ymax], chunk, categorical_variables=[\"Code_18\"])\n",
    "    ests['hindex'] = chunk.hindex.values\n",
    "    ests.to_parquet(f\"../../urbangrammar_samba/spatial_signatures/functional/corine/corine_{chunk_id}.pq\")\n",
    "    print(f\"Chunk {chunk_id} processed sucessfully in {time() - s} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-breakdown",
   "metadata": {},
   "source": [
    "## Retail centres\n",
    "\n",
    "CDRC Retail centres are linked as a distance to the nearest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-repository",
   "metadata": {},
   "outputs": [],
   "source": [
    "retail = gpd.read_file(\"../../urbangrammar_samba/functional_data/retail_centres/Pre Release.zip!Retail_Centres_UK.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-british",
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = 16\n",
    "client = Client(LocalCluster(n_workers=workers, threads_per_worker=1))\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-firewall",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_nearest(chunk):\n",
    "    s = time()\n",
    "    gdf = gpd.read_parquet(f'../../urbangrammar_samba/spatial_signatures/tessellation/tess_{chunk}.pq')\n",
    "    b = gdf.total_bounds\n",
    "    \n",
    "    initial_buffer = 500\n",
    "    buffered = gdf.tessellation.buffer(initial_buffer)\n",
    "    distance = []\n",
    "    for orig, geom in zip(gdf.tessellation, buffered.geometry):\n",
    "        query = retail.sindex.query(geom, predicate='intersects')\n",
    "        b = initial_buffer\n",
    "        while query.size == 0:\n",
    "            query = retail.sindex.query(geom.buffer(b), predicate='intersects')\n",
    "            b += initial_buffer\n",
    "\n",
    "        distance.append(retail.iloc[query].distance(orig).min())\n",
    "    gdf['nearest_retail_centre'] = distance\n",
    "    gdf[['hindex', 'nearest_retail_centre']].to_parquet(f'../../urbangrammar_samba/spatial_signatures/functional/retail_centre/retail_{chunk}.pq')\n",
    "    \n",
    "    return f\"Chunk {chunk} processed sucessfully in {time() - s} seconds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earned-accused",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = iter(range(103))\n",
    "futures = [client.submit(measure_nearest, next(inputs)) for i in range(workers)]\n",
    "ac = as_completed(futures)\n",
    "for finished_future in ac:\n",
    "    # submit new future \n",
    "    try:\n",
    "        new_future = client.submit(measure_nearest, next(inputs))\n",
    "        ac.add(new_future)\n",
    "    except StopIteration:\n",
    "        pass\n",
    "    print(finished_future.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-yield",
   "metadata": {},
   "source": [
    "## Water\n",
    "\n",
    "Water is measured as a distance to the nearest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-lightning",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from shapely.geometry import box\n",
    "from shapely.ops import polygonize\n",
    "\n",
    "user = os.environ.get('DB_USER')\n",
    "pwd = os.environ.get('DB_PWD')\n",
    "host = os.environ.get('DB_HOST')\n",
    "port = os.environ.get('DB_PORT')\n",
    "\n",
    "db_connection_url = f\"postgres+psycopg2://{user}:{pwd}@{host}:{port}/built_env\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-exception",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_nearest(chunk):\n",
    "    s = time()\n",
    "    gdf = gpd.read_parquet(f'../../urbangrammar_samba/spatial_signatures/tessellation/tess_{chunk}.pq')\n",
    "    b = gdf.total_bounds\n",
    "    engine = create_engine(db_connection_url)\n",
    "    sql = f'SELECT * FROM gb_coastline_2016 WHERE ST_Intersects(geometry, ST_MakeEnvelope({b[0]}, {b[1]}, {b[2]}, {b[3]}, 27700))'\n",
    "    coastline = gpd.read_postgis(sql, engine, geom_col='geometry')\n",
    "    sql = f'SELECT * FROM openmap_surfacewater_area_200824 WHERE ST_Intersects(geometry, ST_MakeEnvelope({b[0]}, {b[1]}, {b[2]}, {b[3]}, 27700))'\n",
    "    water = gpd.read_postgis(sql, engine, geom_col='geometry')\n",
    "    \n",
    "    sql = f'SELECT * FROM gb_coastline_2016'\n",
    "    coastline = gpd.read_postgis(sql, engine, geom_col='geometry')\n",
    "\n",
    "    polys = polygonize(coastline.geometry)\n",
    "    land = gpd.GeoSeries(polys, crs=27700)\n",
    "    sea = box(*land.total_bounds).difference(land.geometry.unary_union)\n",
    "    \n",
    "    target = water.geometry\n",
    "    target.loc[len(water)] = sea\n",
    "    target = gpd.clip(target, box(*b))\n",
    "    \n",
    "    initial_buffer = 500\n",
    "    buffered = gdf.tessellation.buffer(initial_buffer)\n",
    "    distance = []\n",
    "    for orig, geom in zip(gdf.tessellation, buffered.geometry):\n",
    "        query = target.sindex.query(geom, predicate='intersects')\n",
    "        b = initial_buffer\n",
    "        while query.size == 0:\n",
    "            query = target.sindex.query(geom.buffer(b), predicate='intersects')\n",
    "            b += initial_buffer\n",
    "\n",
    "        distance.append(target.iloc[query].distance(orig).min())\n",
    "    gdf['nearest_water'] = distance\n",
    "    gdf[['hindex', 'nearest_water']].to_parquet(f'../../urbangrammar_samba/spatial_signatures/functional/water/water_{chunk}.pq')\n",
    "    \n",
    "    return f\"Chunk {chunk} processed sucessfully in {time() - s} seconds.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
