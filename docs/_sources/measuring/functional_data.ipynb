{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "front-exclusion",
   "metadata": {},
   "source": [
    "# Functional data\n",
    "\n",
    "This notebook links various functional layers to ET cells across GB. Various methods are used based on the nature of input data, from areal interpolation to zonal statistics.\n",
    "\n",
    "All data are furhter measured within a relevant spatial context.\n",
    "\n",
    "##Â Population estimates\n",
    "\n",
    "Population estimates are linked using area weighted interpolation based on building geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-mozambique",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import libpysal\n",
    "import tobler\n",
    "from time import time\n",
    "import scipy\n",
    "import xarray\n",
    "import rioxarray\n",
    "import rasterstats\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from dask.distributed import Client, LocalCluster, as_completed\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-hands",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore', message='.*initial implementation of Parquet.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-newark",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_est = gpd.read_parquet(\"../../urbangrammar_samba/functional_data/population_estimates/gb_population_estimates.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-postage",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = gpd.read_parquet(\"../../urbangrammar_samba/spatial_signatures/tessellation/tess_0.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-gregory",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin, ymin, xmax, ymax = chunk.total_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "industrial-accessory",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ests = tobler.area_weighted.area_interpolate(population_est.cx[xmin:xmax, ymin:ymax], chunk.set_geometry(\"buildings\"), extensive_variables=['population'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-favorite",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk_id in range(103):\n",
    "    s = time()\n",
    "    chunk = gpd.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/tessellation/tess_{chunk_id}.pq\", columns=[\"hindex\", \"buildings\"]).set_geometry(\"buildings\")\n",
    "    xmin, ymin, xmax, ymax = chunk.total_bounds\n",
    "    ests = tobler.area_weighted.area_interpolate(population_est.cx[xmin:xmax, ymin:ymax], chunk, extensive_variables=['population'])\n",
    "    pop = pd.DataFrame({'hindex': chunk.hindex.values, \"population\": ests.population.values})\n",
    "    pop.to_parquet(f\"../../urbangrammar_samba/spatial_signatures/functional/population/pop_{chunk_id}\")\n",
    "    print(f\"Chunk {chunk_id} processed sucessfully in {time() - s} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-cameroon",
   "metadata": {},
   "source": [
    "## Night lights\n",
    "\n",
    "Night lights are merged using zonal statistics and parallelisation using `dask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-facing",
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = 8\n",
    "client = Client(LocalCluster(n_workers=workers, threads_per_worker=1))\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-fence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _night_lights(chunk_id):\n",
    "    import rioxarray\n",
    "    \n",
    "    s = time()\n",
    "    \n",
    "    chunk = gpd.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/tessellation/tess_{chunk_id}.pq\", columns=[\"hindex\", \"tessellation\"])\n",
    "    nl = xarray.open_rasterio(\"../../urbangrammar_samba/functional_data/employment/night_lights_osgb.tif\")\n",
    "    nl_clip = nl.rio.clip_box(*chunk.total_bounds)\n",
    "    arr = nl_clip.values\n",
    "    affine = nl_clip.rio.transform()\n",
    "    stats_nl = rasterstats.zonal_stats(\n",
    "        chunk.tessellation, \n",
    "        raster=arr[0],\n",
    "        affine=affine,\n",
    "        stats=['mean'],\n",
    "        all_touched=True,\n",
    "        nodata = np.nan,\n",
    "    )\n",
    "    chunk[\"night_lights\"] = [x['mean'] for x in stats_nl]\n",
    "    chunk[[\"hindex\", \"night_lights\"]].to_parquet(f\"../../urbangrammar_samba/spatial_signatures/functional/night_lights/nl_{chunk_id}\")\n",
    "    \n",
    "    return f\"Chunk {chunk_id} processed sucessfully in {time() - s} seconds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-first",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = iter(range(103))\n",
    "futures = [client.submit(_night_lights, next(inputs)) for i in range(workers)]\n",
    "ac = as_completed(futures)\n",
    "for finished_future in ac:\n",
    "    # submit new future \n",
    "    try:\n",
    "        new_future = client.submit(_night_lights, next(inputs))\n",
    "        ac.add(new_future)\n",
    "    except StopIteration:\n",
    "        pass\n",
    "    print(finished_future.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-facial",
   "metadata": {},
   "source": [
    "## Worplace population by industry\n",
    "\n",
    "Worplace population is linked using area weighted interpolation based on building geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moral-senator",
   "metadata": {},
   "outputs": [],
   "source": [
    "wpz = gpd.read_parquet('../../urbangrammar_samba/functional_data/employment/workplace/workplace_by_industry_gb.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-dealing",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for chunk_id in range(103):\n",
    "    s = time()\n",
    "    chunk = gpd.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/tessellation/tess_{chunk_id}.pq\", columns=[\"hindex\", \"buildings\"]).set_geometry(\"buildings\")\n",
    "    xmin, ymin, xmax, ymax = chunk.total_bounds\n",
    "    ests = tobler.area_weighted.area_interpolate(wpz.cx[xmin:xmax, ymin:ymax], chunk, extensive_variables=wpz.columns[1:-1].to_list())\n",
    "    ests['hindex'] = chunk.hindex.values\n",
    "    ests.drop(columns=\"geometry\").to_parquet(f\"../../urbangrammar_samba/spatial_signatures/functional/workplace/pop_{chunk_id}\")\n",
    "    print(f\"Chunk {chunk_id} processed sucessfully in {time() - s} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-result",
   "metadata": {},
   "source": [
    "## CORINE Land cover\n",
    "\n",
    "CORINE Land cover is linked using area weighted interpolation based on tessellation geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-spectacular",
   "metadata": {},
   "outputs": [],
   "source": [
    "corine = gpd.read_parquet(\"../../urbangrammar_samba/functional_data/land_use/corine/corine_gb.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-guest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dask_binning(corine, cells, n_chunks=512):\n",
    "    import dask_geopandas as dgpd\n",
    "    from scipy.sparse import coo_matrix\n",
    "    \n",
    "    ids_src, ids_tgt = cells.sindex.query_bulk(corine.geometry, predicate=\"intersects\")\n",
    "    df = gpd.GeoDataFrame({'clc': corine.geometry.values[ids_src], 'tess': cells.geometry.values[ids_tgt]})\n",
    "    ddf = dgpd.from_geopandas(df, npartitions=n_chunks)\n",
    "    areas = ddf.clc.intersection(ddf.tess).area.compute()\n",
    "    table = coo_matrix(\n",
    "        (areas, (ids_src, ids_tgt),),\n",
    "        shape=(corine.shape[0], cells.shape[0]),\n",
    "        dtype=np.float32,\n",
    "    )\n",
    "\n",
    "    table = table.todok()\n",
    "\n",
    "    return table\n",
    "\n",
    "\n",
    "def _dask_area_interpolate(corine, cells, n_chunks=512, categorical_variables=None):\n",
    "    table = _dask_binning(corine, cells, n_chunks)\n",
    "    \n",
    "    if categorical_variables:\n",
    "        categorical = {}\n",
    "        for variable in categorical_variables:\n",
    "            unique = corine[variable].unique()\n",
    "            for value in unique:\n",
    "                mask = corine[variable] == value\n",
    "                categorical[f\"{variable}_{value}\"] = np.asarray(\n",
    "                    table[mask].sum(axis=0)\n",
    "                )[0]\n",
    "\n",
    "        categorical = pd.DataFrame(categorical)\n",
    "        categorical = categorical.div(cells.area, axis=\"rows\")\n",
    "    \n",
    "    return categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-manitoba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for chunk_id in range(103):\n",
    "    s = time()\n",
    "    chunk = gpd.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/tessellation/tess_{chunk_id}.pq\", columns=[\"hindex\", \"tessellation\"])\n",
    "    xmin, ymin, xmax, ymax = chunk.total_bounds\n",
    "    ests = _dask_area_interpolate(corine.cx[xmin:xmax, ymin:ymax], chunk, categorical_variables=[\"Code_18\"])\n",
    "    ests['hindex'] = chunk.hindex.values\n",
    "    ests.to_parquet(f\"../../urbangrammar_samba/spatial_signatures/functional/corine/corine_{chunk_id}.pq\")\n",
    "    print(f\"Chunk {chunk_id} processed sucessfully in {time() - s} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-operation",
   "metadata": {},
   "source": [
    "## Retail centres\n",
    "\n",
    "CDRC Retail centres are linked as a distance to the nearest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worse-opera",
   "metadata": {},
   "outputs": [],
   "source": [
    "retail = gpd.read_file(\"../../urbangrammar_samba/functional_data/retail_centres/Pre Release.zip!Retail_Centres_UK.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-understanding",
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = 16\n",
    "client = Client(LocalCluster(n_workers=workers, threads_per_worker=1))\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-palestine",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_nearest(chunk):\n",
    "    s = time()\n",
    "    gdf = gpd.read_parquet(f'../../urbangrammar_samba/spatial_signatures/tessellation/tess_{chunk}.pq')\n",
    "    b = gdf.total_bounds\n",
    "    \n",
    "    initial_buffer = 500\n",
    "    buffered = gdf.tessellation.buffer(initial_buffer)\n",
    "    distance = []\n",
    "    for orig, geom in zip(gdf.tessellation, buffered.geometry):\n",
    "        query = retail.sindex.query(geom, predicate='intersects')\n",
    "        b = initial_buffer\n",
    "        while query.size == 0:\n",
    "            query = retail.sindex.query(geom.buffer(b), predicate='intersects')\n",
    "            b += initial_buffer\n",
    "\n",
    "        distance.append(retail.iloc[query].distance(orig).min())\n",
    "    gdf['nearest_retail_centre'] = distance\n",
    "    gdf[['hindex', 'nearest_retail_centre']].to_parquet(f'../../urbangrammar_samba/spatial_signatures/functional/retail_centre/retail_{chunk}.pq')\n",
    "    \n",
    "    return f\"Chunk {chunk} processed sucessfully in {time() - s} seconds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-grounds",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = iter(range(103))\n",
    "futures = [client.submit(measure_nearest, next(inputs)) for i in range(workers)]\n",
    "ac = as_completed(futures)\n",
    "for finished_future in ac:\n",
    "    # submit new future \n",
    "    try:\n",
    "        new_future = client.submit(measure_nearest, next(inputs))\n",
    "        ac.add(new_future)\n",
    "    except StopIteration:\n",
    "        pass\n",
    "    print(finished_future.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-pressure",
   "metadata": {},
   "source": [
    "## Water\n",
    "\n",
    "Water is measured as a distance to the nearest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-garbage",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from shapely.geometry import box\n",
    "from shapely.ops import polygonize\n",
    "\n",
    "user = os.environ.get('DB_USER')\n",
    "pwd = os.environ.get('DB_PWD')\n",
    "host = os.environ.get('DB_HOST')\n",
    "port = os.environ.get('DB_PORT')\n",
    "\n",
    "db_connection_url = f\"postgres+psycopg2://{user}:{pwd}@{host}:{port}/built_env\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-decline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_nearest(chunk):\n",
    "    s = time()\n",
    "    gdf = gpd.read_parquet(f'../../urbangrammar_samba/spatial_signatures/tessellation/tess_{chunk}.pq')\n",
    "    b = gdf.total_bounds\n",
    "    engine = create_engine(db_connection_url)\n",
    "    sql = f'SELECT * FROM gb_coastline_2016 WHERE ST_Intersects(geometry, ST_MakeEnvelope({b[0]}, {b[1]}, {b[2]}, {b[3]}, 27700))'\n",
    "    coastline = gpd.read_postgis(sql, engine, geom_col='geometry')\n",
    "    sql = f'SELECT * FROM openmap_surfacewater_area_200824 WHERE ST_Intersects(geometry, ST_MakeEnvelope({b[0]}, {b[1]}, {b[2]}, {b[3]}, 27700))'\n",
    "    water = gpd.read_postgis(sql, engine, geom_col='geometry')\n",
    "    \n",
    "    sql = f'SELECT * FROM gb_coastline_2016'\n",
    "    coastline = gpd.read_postgis(sql, engine, geom_col='geometry')\n",
    "\n",
    "    polys = polygonize(coastline.geometry)\n",
    "    land = gpd.GeoSeries(polys, crs=27700)\n",
    "    sea = box(*land.total_bounds).difference(land.geometry.unary_union)\n",
    "    \n",
    "    target = water.geometry\n",
    "    target.loc[len(water)] = sea\n",
    "    target = gpd.clip(target, box(*b))\n",
    "    \n",
    "    initial_buffer = 500\n",
    "    buffered = gdf.tessellation.buffer(initial_buffer)\n",
    "    distance = []\n",
    "    for orig, geom in zip(gdf.tessellation, buffered.geometry):\n",
    "        query = target.sindex.query(geom, predicate='intersects')\n",
    "        b = initial_buffer\n",
    "        while query.size == 0:\n",
    "            query = target.sindex.query(geom.buffer(b), predicate='intersects')\n",
    "            b += initial_buffer\n",
    "\n",
    "        distance.append(target.iloc[query].distance(orig).min())\n",
    "    gdf['nearest_water'] = distance\n",
    "    gdf[['hindex', 'nearest_water']].to_parquet(f'../../urbangrammar_samba/spatial_signatures/functional/water/water_{chunk}.pq')\n",
    "    \n",
    "    return f\"Chunk {chunk} processed sucessfully in {time() - s} seconds.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-academy",
   "metadata": {},
   "source": [
    "## Convolutions\n",
    "\n",
    "Functional characters which do not express the tendency in the spatial context are contextualised using the same method applied to morphometric data - as the 1st, 2nd and 3rd quartile weigted by inverse distance based on cells within 10th order of contiguity. The metdo is applied to:\n",
    "\n",
    "- population\n",
    "- night lights\n",
    "- workplace population\n",
    "- CORINE\n",
    "- NDVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-radar",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_chunk = pd.read_parquet('../../urbangrammar_samba/spatial_signatures/cross-chunk_indices_10.pq')\n",
    "\n",
    "\n",
    "def convolute(chunk_id):\n",
    "    \n",
    "    s = time()\n",
    "    \n",
    "    pop = pd.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/functional/population/pop_{chunk_id}\")\n",
    "    nl = pd.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/functional/night_lights/nl_{chunk_id}\")\n",
    "    workplace = pd.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/functional/workplace/pop_{chunk_id}\")\n",
    "    corine = pd.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/functional/corine/corine_{chunk_id}.pq\")\n",
    "    ndvi = pd.read_parquet(f\"../../urbangrammar_samba/functional_data/ndvi/ndvi_tess_{chunk_id}.pq\")\n",
    "    combined = pop.merge(nl, on='hindex').merge(workplace, on='hindex').merge(corine, on='hindex').merge(ndvi.rename({'mean': 'ndvi'}), on='hindex')\n",
    "    combined['keep'] = True\n",
    "    # add neighbouring cells from other chunks\n",
    "    cross_chunk_cells = []\n",
    "\n",
    "    for chunk, inds in cross_chunk.loc[chunk_id].indices.iteritems():\n",
    "        add_pop = pd.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/functional/population/pop_{chunk}\").iloc[inds]\n",
    "        add_nl = pd.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/functional/night_lights/nl_{chunk}\").iloc[inds]\n",
    "        add_workplace = pd.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/functional/workplace/pop_{chunk}\").iloc[inds]\n",
    "        add_corine = pd.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/functional/corine/corine_{chunk}.pq\").iloc[inds]\n",
    "        add_ndvi = pd.read_parquet(f\"../../urbangrammar_samba/functional_data/ndvi/ndvi_tess_{chunk}.pq\").iloc[inds]\n",
    "        add_combined = add_pop.merge(add_nl, on='hindex').merge(add_workplace, on='hindex').merge(add_corine, on='hindex').merge(add_ndvi.rename({'mean': 'ndvi'}), on='hindex')\n",
    "        add_combined['keep'] = False\n",
    "        cross_chunk_cells.append(add_combined)\n",
    "    \n",
    "    df = combined.append(pd.concat(cross_chunk_cells, ignore_index=True), ignore_index=True).set_index('hindex')\n",
    "\n",
    "    # read W\n",
    "    W = libpysal.weights.WSP(scipy.sparse.load_npz(f\"../../urbangrammar_samba/spatial_signatures/weights/w10_10_distance_circles_{chunk_id}.npz\")).to_W()\n",
    " \n",
    "    characters = df.columns\n",
    "    # prepare dictionary to store results\n",
    "    convolutions = {}\n",
    "    for c in characters:\n",
    "        convolutions[c] = []\n",
    "        \n",
    "    # measure convolutions\n",
    "    for i in range(len(df)):\n",
    "        neighbours = W.neighbors[i]\n",
    "        vicinity = df.iloc[neighbours]\n",
    "        distance = W.weights[i]\n",
    "        distance_decay = 1 / np.array(distance)\n",
    "        \n",
    "        for c in characters:\n",
    "            values = vicinity[c].values\n",
    "            sorter = np.argsort(values)\n",
    "            values = values[sorter]\n",
    "            nan_mask = np.isnan(values)\n",
    "            if nan_mask.all():\n",
    "                convolutions[c].append(np.array([np.nan] * 3))\n",
    "            else:\n",
    "                sample_weight = distance_decay[sorter][~nan_mask]\n",
    "                weighted_quantiles = np.cumsum(sample_weight) - 0.5 * sample_weight\n",
    "                weighted_quantiles /= np.sum(sample_weight)\n",
    "                interpolate = np.interp([.25, .5, .75], weighted_quantiles, values[~nan_mask])\n",
    "                convolutions[c].append(interpolate)\n",
    "    \n",
    "    # save convolutions to parquet file\n",
    "    conv = pd.DataFrame(convolutions, index=df.index)\n",
    "    exploded = pd.concat([pd.DataFrame(conv[c].to_list(), columns=[c + '_q1', c + '_q2',c + '_q3']) for c in characters], axis=1)\n",
    "    convoluted = exploded[df.keep.values]\n",
    "    convoluted['hindex'] = combined['hindex'].values\n",
    "    \n",
    "    pois = pd.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/functional/accessibility/access_{chunk_id}.pq\")\n",
    "    water = pd.read_parquet(f'../../urbangrammar_samba/spatial_signatures/functional/water/water_{chunk_id}.pq')\n",
    "    retail_centres = pd.read_parquet(f'../../urbangrammar_samba/spatial_signatures/functional/retail_centre/retail_{chunk_id}.pq')\n",
    "    \n",
    "    functional = convoluted.merge(pois, on='hindex').merge(water, on='hindex').merge(retail_centres, on='hindex').set_index('hindex')\n",
    "    functional.to_parquet(f\"../../urbangrammar_samba/spatial_signatures/functional/functional/func_{chunk_id}.pq\")\n",
    "    \n",
    "    return f\"Chunk {chunk_id} processed sucessfully in {time() - s} seconds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-diagram",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am afraid that we would run out of memory if we did this in parallel\n",
    "for i in tqdm(range(103), total=103):\n",
    "    print(convolute(i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
