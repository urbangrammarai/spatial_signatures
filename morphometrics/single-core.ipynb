{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "import libpysal\n",
    "import scipy\n",
    "\n",
    "from dask.distributed import Client, LocalCluster, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:39469</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>8</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>84.28 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:39469' processes=8 threads=8, memory=84.28 GB>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workers = 8\n",
    "client = Client(LocalCluster(n_workers=workers, threads_per_worker=1))\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_chunk = pd.read_parquet('../../urbangrammar_samba/spatial_signatures/cross-chunk_indices.pq')\n",
    "# chunks = geopandas.read_parquet('../../urbangrammar_samba/spatial_signatures/local_auth_chunks.pq')\n",
    "\n",
    "# user = os.environ.get('DB_USER')\n",
    "# pwd = os.environ.get('DB_PWD')\n",
    "# host = os.environ.get('DB_HOST')\n",
    "# port = os.environ.get('DB_PORT')\n",
    "\n",
    "# db_connection_url = f\"postgres+psycopg2://{user}:{pwd}@{host}:{port}/built_env\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(chunk_id):\n",
    "    # load cells of a chunk\n",
    "    cells = geopandas.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/cells/cells_{chunk_id}.pq\")\n",
    "    cells['keep'] = True\n",
    "    \n",
    "    # add neighbouring cells from other chunks\n",
    "    cross_chunk_cells = []\n",
    "    \n",
    "    for chunk, inds in cross_chunk.loc[chunk_id].indices.iteritems():\n",
    "        add_cells = geopandas.read_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/cells/cells_{chunk}.pq\").iloc[inds]\n",
    "        add_cells['keep'] = False\n",
    "        cross_chunk_cells.append(add_cells)\n",
    "    \n",
    "    df = cells.append(pd.concat(cross_chunk_cells, ignore_index=True), ignore_index=True)\n",
    "\n",
    "    # read W\n",
    "    w = libpysal.weights.WSP(scipy.sparse.load_npz(f\"../../urbangrammar_samba/spatial_signatures/weights/w_{chunk_id}.npz\")).to_W()\n",
    "    \n",
    "    # alignment\n",
    "    def alignment(x, orientation='stbOri'):\n",
    "        orientations = df[orientation].iloc[w.neighbors[x]]\n",
    "        return abs(orientations - df[orientation].iloc[x]).mean()\n",
    "    \n",
    "    df['mtbAli'] = [alignment(x) for x in range(len(df))]\n",
    "\n",
    "    # mean neighbour distance\n",
    "    def neighbor_distance(x):\n",
    "        geom = df.buildings.iloc[x]\n",
    "        if geom is None:\n",
    "            return np.nan\n",
    "        return df.buildings.iloc[w.neighbors[x]].distance(df.buildings.iloc[x]).mean()\n",
    "\n",
    "    df['mtbNDi'] = [neighbor_distance(x) for x in range(len(df))]\n",
    "    \n",
    "    # weighted neighbours\n",
    "    df['mtcWNe'] = pd.Series([w.cardinalities[x] for x in range(len(df))], index=df.index) / df.tessellation.length\n",
    "    \n",
    "    # area covered by neighbours\n",
    "    def area_covered(x, area='sdcAre'):\n",
    "        neighbours = [x]\n",
    "        neighbours += w.neighbors[x]\n",
    "\n",
    "        return df[area].iloc[neighbours].sum()\n",
    "\n",
    "    df['mdcAre'] = [area_covered(x) for x in range(len(df))]\n",
    "    \n",
    "    # read W3 here\n",
    "    w3 = libpysal.weights.WSP(scipy.sparse.load_npz(f\"../../urbangrammar_samba/spatial_signatures/weights/w3_{chunk_id}.npz\")).to_W()\n",
    "      \n",
    "    # weighted reached enclosures\n",
    "    def weighted_reached_enclosures(x, area='sdcAre', enclosure_id='enclosureID'):\n",
    "        neighbours = [x]\n",
    "        neighbours += w3.neighbors[x]\n",
    "\n",
    "        vicinity = df[[area, enclosure_id]].iloc[neighbours]\n",
    "\n",
    "        return vicinity[enclosure_id].unique().shape[0] / vicinity[area].sum()\n",
    "    \n",
    "    df['ltcWRE'] = [weighted_reached_enclosures(x) for x in range(len(df))]\n",
    "    \n",
    "    # mean interbuilding distance - it takes ages\n",
    "    # define adjacency list from lipysal\n",
    "    adj_list = w.to_adjlist(remove_symmetric=True)\n",
    "    adj_list[\"distance\"] = (\n",
    "        df.buildings.iloc[adj_list.focal]\n",
    "        .reset_index(drop=True)\n",
    "        .distance(df.buildings.iloc[adj_list.neighbor].reset_index(drop=True))\n",
    "    )\n",
    "    adj_list = adj_list.set_index(['focal', 'neighbor'])\n",
    "\n",
    "\n",
    "    def mean_interbuilding_distance(x):\n",
    "        neighbours = [x]\n",
    "        neighbours += w3.neighbors[x]\n",
    "        return adj_list.distance.loc[neighbours, neighbours].mean()\n",
    "\n",
    "\n",
    "    df['ltbIBD'] = [mean_interbuilding_distance(x) for x in range(len(df))]\n",
    "\n",
    "    df[df['keep']].drop(columns=['keep']).to_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/cells/cells_{chunk_id}.pq\")\n",
    "\n",
    "#     chunk_area = chunks.geometry.iloc[chunk_id].buffer(5000)\n",
    "#     engine = create_engine(db_connection_url)\n",
    "#     sql = f\"SELECT * FROM openroads_200803_topological WHERE ST_Intersects(geometry, ST_GeomFromText('{chunk_area.wkt}',27700))\"\n",
    "#     streets = geopandas.read_postgis(sql, engine, geom_col='geometry')\n",
    "    \n",
    "#     sp = street_profile(streets, blg)\n",
    "#     streets['sdsSPW'] = sp[0]\n",
    "#     streets['sdsSWD'] = sp[1]\n",
    "#     streets['sdsSPO'] = sp[2]\n",
    "    \n",
    "#     streets['sdsLen'] = streets.length\n",
    "#     streets['sssLin'] = momepy.Linearity(streets).series\n",
    "    \n",
    "#     G = momepy.gdf_to_nx(streets)\n",
    "#     G = momepy.node_degree(G)\n",
    "#     G = momepy.subgraph(\n",
    "#         G,\n",
    "#         radius=5,\n",
    "#         meshedness=True,\n",
    "#         cds_length=False,\n",
    "#         mode=\"sum\",\n",
    "#         degree=\"degree\",\n",
    "#         length=\"mm_len\",\n",
    "#         mean_node_degree=False,\n",
    "#         proportion={0: True, 3: True, 4: True},\n",
    "#         cyclomatic=False,\n",
    "#         edge_node_ratio=False,\n",
    "#         gamma=False,\n",
    "#         local_closeness=True,\n",
    "#         closeness_weight=\"mm_len\",\n",
    "#         verbose=False\n",
    "#     )\n",
    "#     G = momepy.cds_length(G, radius=3, name=\"ldsCDL\", verbose=False)\n",
    "#     G = momepy.clustering(G, name=\"xcnSCl\")\n",
    "#     G = momepy.mean_node_dist(G, name=\"mtdMDi\", verbose=False)\n",
    "    \n",
    "#     nodes, edges, sw = momepy.nx_to_gdf(G, spatial_weights=True)\n",
    "    \n",
    "#     edges_w3 = momepy.sw_high(k=3, gdf=edges)\n",
    "    \n",
    "#     edges[\"ldsMSL\"] = momepy.SegmentsLength(edges, spatial_weights=edges_w3, mean=True, verbose=False).series\n",
    "    \n",
    "#     nodes_w5 = momepy.sw_high(k=5, weights=sw)\n",
    "    \n",
    "#     nodes[\"lddNDe\"] = momepy.NodeDensity(nodes, edges, nodes_w5, verbose=False).series\n",
    "    \n",
    "#     nodes[\"linWID\"] = momepy.NodeDensity(nodes, edges, nodes_w5, weighted=True, node_degree=\"degree\", verbose=False).series\n",
    "    \n",
    "#     edges.to_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/edges/edges_{chunk_id}.pq\")\n",
    "#     nodes.to_parquet(f\"../../urbangrammar_samba/spatial_signatures/morphometrics/nodes/nodes_{chunk_id}.pq\")\n",
    "\n",
    "\n",
    "    return f\"Chunk {chunk_id} processed sucessfully.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 28 processed sucessfully.\n",
      "Chunk 34 processed sucessfully.\n",
      "Chunk 29 processed sucessfully.\n",
      "Chunk 31 processed sucessfully.\n",
      "Chunk 33 processed sucessfully.\n",
      "Chunk 35 processed sucessfully.\n",
      "Chunk 30 processed sucessfully.\n",
      "Chunk 36 processed sucessfully.\n",
      "Chunk 39 processed sucessfully.\n",
      "Chunk 37 processed sucessfully.\n",
      "Chunk 41 processed sucessfully.\n",
      "Chunk 43 processed sucessfully.\n",
      "Chunk 32 processed sucessfully.\n",
      "Chunk 42 processed sucessfully.\n",
      "Chunk 44 processed sucessfully.\n",
      "Chunk 47 processed sucessfully.\n",
      "Chunk 45 processed sucessfully.\n",
      "Chunk 46 processed sucessfully.\n",
      "Chunk 48 processed sucessfully.\n",
      "Chunk 38 processed sucessfully.\n",
      "Chunk 49 processed sucessfully.\n",
      "Chunk 51 processed sucessfully.\n",
      "Chunk 50 processed sucessfully.\n",
      "Chunk 52 processed sucessfully.\n",
      "Chunk 40 processed sucessfully.\n",
      "Chunk 55 processed sucessfully.\n",
      "Chunk 53 processed sucessfully.\n",
      "Chunk 54 processed sucessfully.\n",
      "Chunk 56 processed sucessfully.\n",
      "Chunk 57 processed sucessfully.\n",
      "Chunk 58 processed sucessfully.\n",
      "Chunk 60 processed sucessfully.\n",
      "Chunk 59 processed sucessfully.\n",
      "Chunk 63 processed sucessfully.\n",
      "Chunk 62 processed sucessfully.\n",
      "Chunk 65 processed sucessfully.\n",
      "Chunk 68 processed sucessfully.\n",
      "Chunk 66 processed sucessfully.\n",
      "Chunk 69 processed sucessfully.\n",
      "Chunk 70 processed sucessfully.\n",
      "Chunk 72 processed sucessfully.\n",
      "Chunk 64 processed sucessfully.\n",
      "Chunk 71 processed sucessfully.\n",
      "Chunk 61 processed sucessfully.\n",
      "Chunk 67 processed sucessfully.\n",
      "Chunk 74 processed sucessfully.\n",
      "Chunk 73 processed sucessfully.\n",
      "Chunk 79 processed sucessfully.\n",
      "Chunk 78 processed sucessfully.\n",
      "Chunk 81 processed sucessfully.\n",
      "Chunk 77 processed sucessfully.\n",
      "Chunk 75 processed sucessfully.\n",
      "Chunk 76 processed sucessfully.\n",
      "Chunk 80 processed sucessfully.\n",
      "Chunk 84 processed sucessfully.\n",
      "Chunk 82 processed sucessfully.\n",
      "Chunk 83 processed sucessfully.\n",
      "Chunk 87 processed sucessfully.\n",
      "Chunk 86 processed sucessfully.\n",
      "Chunk 90 processed sucessfully.\n",
      "Chunk 88 processed sucessfully.\n",
      "Chunk 85 processed sucessfully.\n",
      "Chunk 91 processed sucessfully.\n",
      "Chunk 93 processed sucessfully.\n",
      "Chunk 94 processed sucessfully.\n",
      "Chunk 96 processed sucessfully.\n",
      "Chunk 97 processed sucessfully.\n",
      "Chunk 95 processed sucessfully.\n",
      "Chunk 92 processed sucessfully.\n",
      "Chunk 100 processed sucessfully.\n",
      "Chunk 99 processed sucessfully.\n",
      "Chunk 89 processed sucessfully.\n",
      "Chunk 102 processed sucessfully.\n",
      "Chunk 98 processed sucessfully.\n",
      "Chunk 101 processed sucessfully.\n"
     ]
    }
   ],
   "source": [
    "inputs = iter(range(28, 103))\n",
    "futures = [client.submit(measure, next(inputs)) for i in range(workers)]\n",
    "ac = as_completed(futures)\n",
    "for finished_future in ac:\n",
    "    # submit new future \n",
    "    try:\n",
    "        new_future = client.submit(measure, next(inputs))\n",
    "        ac.add(new_future)\n",
    "    except StopIteration:\n",
    "        pass\n",
    "    print(finished_future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tracemalloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 4 disconnected components.\n",
      " There are 2 islands with ids: 157186, 164512.\n",
      "  warnings.warn(message)\n",
      "/opt/conda/lib/python3.7/site-packages/libpysal/weights/weights.py:172: UserWarning: The weights matrix is not fully connected: \n",
      " There are 4 disconnected components.\n",
      " There are 2 islands with ids: 157186, 164512.\n",
      "  warnings.warn(message)\n",
      "/opt/conda/lib/python3.7/site-packages/libpysal/weights/weights.py:309: UserWarning: {} islands in this weights matrix. Conversion to an adjacency list will drop these observations!\n",
      "  \"{} islands in this weights matrix. Conversion to an \"\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:80: UserWarning: this is an initial implementation of Parquet/Feather file support and associated metadata.  This is tracking version 0.1.0 of the metadata specification at https://github.com/geopandas/geo-arrow-spec\n",
      "\n",
      "This metadata specification does not yet make stability promises.  We do not yet recommend using this in a production setting unless you are able to rewrite your Parquet/Feather files.\n",
      "\n",
      "To further ignore this warning, you can do: \n",
      "import warnings; warnings.filterwarnings('ignore', message='.*initial implementation of Parquet.*')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory usage is 157.117712MB; Peak was 1419.861516MB\n",
      "CPU times: user 4h 2min 5s, sys: 6min 51s, total: 4h 8min 56s\n",
      "Wall time: 3h 47min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tracemalloc.start()\n",
    "\n",
    "ret = measure(26)\n",
    "\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "print(f\"Current memory usage is {current / 10**6}MB; Peak was {peak / 10**6}MB\")\n",
    "tracemalloc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracemalloc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current memory usage is 56.681588MB; Peak was 1160.209484MB\n",
    "CPU times: user 11min 40s, sys: 53.6 s, total: 12min 34s\n",
    "Wall time: 11min 15s\n",
    "    \n",
    "    Excluding IBD\n",
    "    \n",
    "\n",
    "Current memory usage is 38.199543MB; Peak was 1145.271618MB\n",
    "CPU times: user 1h 41min 3s, sys: 7.36 s, total: 1h 41min 11s\n",
    "Wall time: 1h 41min 16s\n",
    "\n",
    "    Including IBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
